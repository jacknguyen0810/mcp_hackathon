<!-- image -->

## Diffusion model

In machine learning, diffusion models , also known as diffusion probabilistic models or scorebased generative models , are a class of latent variable generative models. A diffusion model consists of three major components: the forward process, the reverse process, and the sampling procedure. [1] The goal of diffusion models is to learn a diffusion process for a given dataset, such that the process can generate new elements that are distributed similarly as the original dataset. A diffusion model models data as generated by a diffusion process, whereby a new datum performs a random walk with drift through the space of all possible data. [2] A trained diffusion model can be sampled in many ways, with different efficiency and quality.

There are various equivalent formalisms, including Markov chains, denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. [3] They are typically trained   using   variational   inference. [4] The   model   responsible   for   denoising   is   typically   called   its "backbone". The backbone may be of any kind, but they are typically U-nets or transformers.

As of 2024, diffusion models are mainly used for computer vision tasks, including image denoising, inpainting, super-resolution, image generation, and video generation. These typically involve training a neural network to sequentially denoise images blurred with Gaussian noise. [2][5] The model is trained to reverse the process of adding noise to an image. After training to convergence, it can be used for image generation by starting with an image composed of random noise, and applying the network iteratively to denoise the image.

Diffusion-based image generators have seen widespread commercial interest, such as Stable Diffusion and DALL-E. These models typically combine diffusion models with other models, such as text-encoders and cross-attention modules to allow text-conditioned generation. [6]

Other   than   computer   vision,   diffusion   models   have   also   found   applications   in   natural   language processing [7][8] such   as   text   generation [9][10] and   summarization, [11] sound   generation, [12] and reinforcement learning. [13][14]

## Denoising diffusion model

## Non-equilibrium thermodynamics

Diffusion models were introduced in 2015 as a method to train a model that can sample from a highly complex   probability   distribution.   They   used   techniques   from   non-equilibrium   thermodynamics, especially diffusion. [15]

Consider, for example, how one might model the distribution of all naturally-occurring photos. Each image is a point in the space of all images, and the distribution of naturally-occurring photos is a "cloud" in space, which, by repeatedly adding noise to the images, diffuses out to the rest of the image space, until the cloud becomes all but indistinguishable from a Gaussian distribution .   A   model that can approximately undo the diffusion can then be used to sample from the original distribution. This is

studied in "non-equilibrium" thermodynamics, as the starting distribution is not in equilibrium, unlike the final distribution.

The equilibrium distribution is the Gaussian distribution , with pdf . This is just the Maxwell-Boltzmann distribution of particles in a potential well at temperature 1. The initial   distribution,   being   very   much   out   of   equilibrium,   would   diffuse   towards   the   equilibrium distribution, making biased random steps that are a sum of pure randomness (like a Brownian walker) and gradient descent down the potential well. The randomness is necessary: if the particles were to undergo only gradient descent, then they will all fall to the origin, collapsing the distribution.

## Denoising Diffusion Probabilistic Model (DDPM)

The 2020 paper proposed the Denoising Diffusion Probabilistic Model (DDPM), which improves upon the previous method by variational inference. [4][16]

## Forward diffusion

To present the model, we need some notation.

- ▪ are fixed constants.
- ▪
- ▪
- ▪
- ▪

<!-- formula-not-decoded -->

- ▪ is the normal distribution with mean and variance , and is the probability density at .
- ▪ A vertical bar denotes conditioning.

A forward diffusion process starts   at   some   starting   point ,   where is   the   probability distribution to be learned, then repeatedly adds noise to it by where are IID samples from . This is designed so that for any starting distribution of , we have converging to .

The entire diffusion process then satisfies or

where is a normalization constant and often omitted. In particular, we note that is a gaussian process,   which   affords   us   considerable   freedom   in   reparameterization.   For   example,   by   standard manipulation with gaussian process,

In particular, notice that for large , the variable converges to . That is, after a long enough diffusion process, we end up with some that is very close to ,   with all traces of the original gone.

For example, since we can  sample directly   "in   one   step",   instead   of   going   through   all   the   intermediate   steps .

## Derivation by reparameterization

We know is a gaussian, and is another gaussian. We also know that these are independent. Thus we can perform a reparameterization:

where are IID gaussians.

There are 5 variables and   two   linear   equations.   The   two   sources   of randomness are ,   which can be reparameterized by rotation, since the IID gaussian distribution is rotationally symmetric.

By plugging in the equations, we can solve for the first reparameterization:

where is a gaussian with mean zero and variance one.

To find the second one, we complete the rotational matrix:

Since rotational matrices are all of the form , we know the matrix must be

and since the inverse of rotational matrix is its transpose,

Plugging back, and simplifying, we have

## Backward diffusion

The key idea of DDPM is to use a neural network parametrized by . The network takes in two arguments , and outputs a vector and a matrix , such that each step in the forward diffusion process can be approximately undone by . This then gives us a backward diffusion process defined by

The goal now is to learn the parameters such that is as close to as possible. To do that, we use maximum likelihood estimation with variational inference.

## Variational inference

The ELBO inequality states that ,   and taking one more expectation, we get

We see that maximizing the quantity on the right would give us a lower bound on the likelihood of observed data. This allows us to perform variational inference.

Define the loss function and now the goal is to minimize the loss by stochastic gradient descent. The expression may be simplified to [17]

<!-- image -->

where does not depend on the parameter, and thus can be ignored. Since also does not depend on the parameter, the term can also be ignored. This

<!-- formula-not-decoded -->

## Noise prediction network

Since ,   this   suggests   that   we   should   use ; however, the network does not have access to ,   and   so   it   has   to   estimate   it   instead.   Now,   since , we may write , where is some unknown gaussian noise. Now we see that estimating is equivalent to estimating .

Therefore, let the network output a noise vector , and let it predict

It remains to design . The DDPM paper suggested not learning it (since it resulted in "unstable training   and   poorer   sample   quality"),   but   fixing   it   at   some   value ,   where   either yielded similar performance.

With this, the loss simplifies to which may be minimized by stochastic gradient descent. The paper noted empirically that an even simpler loss function

resulted in better models.

## Backward diffusion process

After a noise prediction network is trained, it can be used for generating data points in the original distribution in a loop as follows:

- 1. Compute the noise estimate
- 2. Compute the original data estimate
- 3. Sample the previous data
- 4. Change time

## Score-based generative model

Score-based generative model is another formulation of diffusion modelling. They are also called noise conditional score network (NCSN) or score-matching with Langevin dynamics (SMLD). [18][19][20][21]

## Score matching

## The idea of score functions

Consider the problem of image generation. Let represent an image, and let be the probability distribution over all possible images. If we have itself, then we can say for certain how likely a certain

image is. However, this is intractable in general.

Most often, we are uninterested in knowing the absolute probability of a certain image. Instead, we are usually only interested in knowing how likely a certain image is compared to its immediate neighbors e.g. how much more likely is an image of cat compared to some small variants of it? Is it more likely if the image contains two whiskers, or three, or with some Gaussian noise added?

Consequently, we are actually quite uninterested in itself, but rather, . This has two major effects:

- ▪ One, we no longer need to normalize , but can use any , where
- is any unknown constant that is of no concern to us.
- ▪ Two, we are comparing neighbors , by

Let the score function be ; then consider what we can do with .

As it turns out, allows us to sample from using thermodynamics. Specifically, if we have a potential   energy   function ,   and   a   lot   of   particles   in   the   potential   well,   then   the distribution at thermodynamic equilibrium is the Boltzmann distribution . At temperature , the Boltzmann distribution is exactly .

Therefore, to model , we may start with a particle sampled at any convenient distribution (such as the standard gaussian distribution), then simulate the motion of the particle forwards according to the Langevin equation and the Boltzmann distribution is, by Fokker-Planck equation, the unique thermodynamic equilibrium. So no matter what distribution has, the distribution of converges in distribution to as .

## Learning the score function

Given a density , we wish to learn a score function approximation . This is score matching . [22] Typically,   score   matching   is   formalized   as   minimizing Fisher   divergence function . By expanding the integral, and performing an integration by parts, giving us a loss function, also known as the Hyvärinen scoring rule, that can be minimized by stochastic gradient descent.

## Annealing the score function

Suppose we need to model the distribution of images, and we want , a white-noise image. Now,   most   white-noise   images   do   not   look   like   real   images,   so for   large   swaths   of . This presents a problem for learning the score function, because if there are no samples around a certain point, then we can't learn the score function at that point. If we do not know the score function at that point, then we cannot impose the time-evolution equation on a particle:

To deal with this problem, we perform annealing. If is too different from a white-noise distribution, then progressively add noise until it is indistinguishable from one. That is, we perform a forward diffusion, then learn the score function, then use the score function to perform a backward diffusion.

## Continuous diffusion processes

## Forward diffusion process

Consider again the forward diffusion process, but this time in continuous time:

By taking the limit, we obtain a continuous diffusion process, in the form of a stochastic differential equation:

where is a Wiener process (multidimensional Brownian motion).

Now, the equation is exactly a special case of the overdamped Langevin equation where is   diffusion   tensor, is   temperature,  and is   potential   energy   field.   If   we   substitute   in ,   we   recover   the   above   equation.   This   explains   why   the   phrase "Langevin dynamics" is sometimes used in diffusion models.

Now the above equation is for the stochastic motion of a single particle. Suppose we have a cloud of particles distributed according to at time , then after a long time, the cloud of particles would settle into the stable distribution of . Let be the density of the cloud of particles at time , then we have and the goal is to somehow reverse the process, so that we can start at the end and diffuse back to the beginning.

By Fokker-Planck equation, the density of the cloud evolves according to where is the dimension of space, and is the Laplace operator. Equivalently,

## Backward diffusion process

If we have solved for time , then we can exactly reverse the evolution of the cloud. Suppose we

start with another cloud of particles with density ,   and   let   the   particles   in   the   cloud   evolve according to then by plugging into the Fokker-Planck equation, we find that . Thus this cloud of points is the original cloud, evolving backwards. [23]

## Noise conditional score network (NCSN)

At the continuous limit, and so

In particular, we see that we can directly sample from any point in the continuous diffusion process without   going   through   the   intermediate   steps,   by   first   sampling ,   then   get . That is, we can quickly sample for any .

Now, define a certain probability distribution over ,   then the score-matching loss function is defined as the expected Fisher divergence:

After training, , so we can perform the backwards diffusion process by first sampling , then integrating the SDE from to :

This may be done by any SDE integration method, such as Euler-Maruyama method.

The name "noise conditional score network" is explained thus:

- ▪ "network", because is implemented as a neural network.
- ▪ "score", because the output of the network is interpreted as approximating the score function .
- ▪ "noise conditional", because is equal to blurred by an added gaussian noise that increases with time, and so the score function depends on the amount of noise added.

## Their equivalence

DDPM and score-based generative models are equivalent. [19][2][24] This means that a network trained using DDPM can be used as a NCSN, and vice versa.

We know that , so by Tweedie's formula, we have

As described previously, the DDPM loss function is with where . By a change of variables,

and the term inside becomes a least squares regression, so if the network actually reaches the global minimum of loss, then we have

Thus, a score-based network predicts noise, and can be used for denoising.

Conversely, the continuous limit of the backward equation gives us precisely the same equation as score-based diffusion:

Thus, at infinitesimal steps of DDPM, a denoising network performs score-based diffusion.

## Main variants

## Noise schedule

In DDPM, the sequence of numbers is   called   a   (discrete   time) noise schedule . In general, consider a strictly increasing monotonic function of type , such as the sigmoid function. In that case, a noise schedule is a sequence of real numbers . It then   defines   a   sequence   of   noises , which   then   derives   the   other   quantities .

In order to use arbitrary noise schedules, instead of training a noise prediction model , one trains .

Similarly, for the noise conditional score network, instead of training , one trains .

## Denoising Diffusion Implicit Model (DDIM)

The original DDPM method for generating images is slow, since the forward diffusion process usually takes to make the distribution   of to   appear   close   to   gaussian.   However   this means  the   backward   diffusion   process   also   take   1000   steps. Unlike the forward diffusion process, which can skip steps as is   gaussian for all ,   the   backward diffusion process does   not   allow   skipping   steps.   For   example,   to   sample requires   the model to first sample . Attempting to directly sample would require us to marginalize out ,   which   is   generally intractable.

Illustration for a linear diffusion noise schedule. With settings

<!-- image -->

.

DDIM [25] is a method to take any model trained on DDPM loss, and use it to sample with some steps skipped, sacrificing an adjustable amount of quality. If we generate the Markovian chain case in DDPM to non-Markovian case, DDIM corresponds to the case that the reverse process has variance equals to 0. In other words, the reverse process (and also the forward process) is deterministic. When using fewer sampling steps, DDIM outperforms DDPM.

In   detail,   the   DDIM   sampling   method   is   as   follows.   Start   with   the   forward   diffusion   process . Then, during the backward denoising process, given , the original data is estimated as then the backward diffusion process can jump to any step , and the next denoised sample is

where is   an   arbitrary real number within the range ,   and is   a   newly   sampled gaussian noise. [17] If all , then the backward process becomes deterministic, and this special case of DDIM is also called "DDIM". The original paper noted that when the process is deterministic, samples generated with only 20 steps are already very similar to ones generated with 1000 steps on the high-level.

The original paper recommended defining a single "eta value" ,   such that .   When , this is the original DDPM. When , this is the fully deterministic DDIM. For intermediate values, the process interpolates between them.

By the equivalence, the DDIM algorithm also applies for score-based diffusion models.

## Latent diffusion model (LDM)

Since the diffusion model is a general method for modelling probability distributions, if one wants to model a distribution over images, one can first encode the images into a lower-dimensional space by an encoder, then use a diffusion model to model the distribution over encoded images. Then to generate an image, one can sample from the diffusion model, then use a decoder to decode it into an image. [26]

The encoder-decoder pair is most often a variational autoencoder (VAE).

## Architectural improvements

[27] proposed various architectural improvements. For example, they proposed log-space interpolation during backward sampling. Instead of sampling from ,   they   recommended sampling from for a learned parameter .

In the v-prediction formalism, the noising formula is reparameterised by an angle such that and a "velocity" defined by . The network is trained to   predict   the   velocity , and   denoising   is   by . [28] This parameterization was found to improve performance, as the model can be trained to reach total noise (i.e. ) and then reverse it, whereas the standard parameterization never reaches total noise since is always true. [29]

## Classifier guidance

Classifier guidance was proposed in 2021 to improve class-conditional generation by using a classifier. The original publication used CLIP text encoders to improve text-conditional image generation. [30]

Suppose we wish to sample not from the entire distribution of images, but conditional on the image description. We don't want to sample a generic image, but an image that fits the description "black cat with red eyes". Generally, we want to sample from the distribution , where ranges over images, and ranges over classes of images (a description "black cat with red eyes" is just a very detailed class, and a class "cat" is just a very vague description).

Taking the perspective of the noisy channel model, we can understand the process as follows: To generate an image conditional on description , we imagine that the requester really had in mind an image , but the image is passed through a noisy channel and came out garbled, as . Image generation is then nothing but inferring which the requester had in mind.

In other words, conditional image generation is simply "translating from a textual language into a pictorial language". Then, as in noisy-channel model, we use Bayes theorem to get in other words, if we have a good model of the space of all images, and a good image-to-class translator, we get a class-to-image translator "for free". In the equation for backward diffusion, the score can be replaced by

where is the score function, trained as previously described, and is   found by using a differentiable image classifier.

During the diffusion process, we need to condition on the time, giving

Although, usually the classifier model does not depend on time, in which case .

Classifier guidance is defined for the gradient of score function, thus for score-based diffusion network,

but   as   previously   noted,   score-based   diffusion   models   are   equivalent   to   denoising   models   by ,   and   similarly, .   Therefore,   classifier guidance works for denoising diffusion as well, using the modified noise prediction: [30]

## With temperature

The classifier-guided diffusion model samples from , which is concentrated around the maximum a posteriori estimate .   If   we   want   to   force   the   model   to   move   towards   the   maximum likelihood estimate , we can use where is interpretable as inverse temperature .   In the context of diffusion models, it is usually called the guidance scale . A high would force the model to sample from a distribution concentrated around . This sometimes improves quality of generated images. [30]

This gives a modification to the previous equation:

For denoising models, it corresponds to [31]

## Classifier-free guidance (CFG)

If we do not have a classifier , we could still extract one out of the image model itself: [31]

Such a model is usually trained by presenting it with both and , allowing it to model both and .

Note that for CFG, the diffusion model cannot be merely a generative model of the entire data distribution . It must be a conditional generative model . For example, in stable diffusion, the diffusion backbone takes as input both a noisy model , a time , and a conditioning vector (such as a vector encoding a text prompt), and produces a noise prediction .

For denoising models, it corresponds to

As sampled by DDIM, the algorithm can be written as [32]

<!-- image -->

A   similar   technique   applies   to   language   model   sampling.   Also,   if   the   unconditional   generation is replaced by , then it results in negative prompting, which pushes the generation away from condition. [33][34]

## Samplers

Given a diffusion model, one may regard it either as a continuous process, and sample from it by integrating a SDE, or one can regard it as a discrete process, and sample from it by iterating the discrete steps. The choice of the " noise schedule " can also affect the quality of samples. A noise schedule is a function that sends a natural number to a noise level:

A noise schedule is more often specified by a map .   The two definitions are equivalent, since

<!-- formula-not-decoded -->

In the DDPM perspective, one can use the DDPM itself (with noise), or DDIM (with adjustable amount of noise). The case where one adds noise is sometimes called ancestral sampling. [35] One can interpolate between noise and no noise. The amount of noise is denoted ("eta value") in the DDIM paper, with denoting no noise (as in deterministic DDIM), and denoting full noise (as in DDPM).

In the perspective of SDE, one can use any of the numerical integration methods, such as EulerMaruyama method, Heun's method, linear multistep methods, etc. Just as in the discrete case, one can add an adjustable amount of noise during the integration. [36]

A survey and comparison of samplers in the context of image generation is in. [37]

## Other examples

Notable variants include [38] Poisson flow generative model, [39] consistency model, [40] critically-damped Langevin diffusion, [41] GenPhys, [42] cold diffusion, [43] discrete diffusion, [44][45] etc.

## Flow-based diffusion model

Abstractly speaking, the idea of diffusion model is to take an unknown probability distribution (the distribution of natural-looking images), then progressively convert it to a known probability distribution (standard gaussian distribution), by building an absolutely continuous probability path connecting them. The probability path is in fact defined implicitly by the score function .

In denoising diffusion models, the forward process adds noise, and the backward process removes noise. Both the forward and backward processes are SDEs, though the forward process is integrable in closedform, so it can be done at no computational cost. The backward process is not integrable in closed-form, so it   must   be   integrated   step-by-step   by   standard   SDE   solvers,   which   can   be   very   expensive.   The probability   path   in   diffusions   model   is   defined   through   an   Itô   process   and   one   can   retrieve   the deterministic process by using the Probability ODE flow formulation. [2]

In flow-based diffusion models, the forward process is a deterministic flow along a time-dependent vector field,   and   the   backward   process   is   also   a   deterministic   flow   along   the   same   vector   field,   but   going backwards. Both processes are solutions to ODEs. If the vector field is well-behaved, the ODE will also be well-behaved.

Given two distributions and ,   a   flow-based   model   is   a   time-dependent   velocity   field in , such that if we start by sampling a point , and let it move according to the velocity field:

we  end   up   with   a   point .   The   solution of   the   above   ODE   define   a   probability   path by the pushforward measure operator. In particular, .

The probability path and the velocity field also satisfy the continuity equation, in the sense of probability distribution:

To construct a probability path, we start by construct a conditional probability path and the corresponding conditional velocity field on some conditional distribution . A natural choice is the Gaussian conditional probability path:

The conditional velocity field which corresponds to the geodesic path between conditional Gaussian path is

The probability path and velocity field are then computed by marginalizing

## Optimal transport flow

The idea of optimal transport flow [46] is to construct a probability path minimizing the Wasserstein metric. The distribution on which we condition is an approximation of the optimal transport plan between and : and , where is the optimal transport plan, which can be approximated by mini-batch optimal transport. If the batch size is not large, then the transport it computes can be very far from the true optimal transport.

## Rectified flow

The idea of rectified flow [47][48] is to learn a flow model such that the velocity is nearly constant along each flow path. This is beneficial, because we can integrate along such a vector field with very few steps. For   example,   if   an   ODE follows   perfectly   straight   paths,   it   simplifies   to ,   allowing for exact solutions in one step. In practice, we cannot reach such

perfection, but when the flow field is nearly so, we can take a few large steps instead of many little steps.

<!-- image -->

The   general   idea   is   to   start   with   two   distributions and , then   construct   a   flow   field from it, then repeatedly apply a "reflow" operation to obtain successive flow fields ,   each   straighter   than   the   previous   one.   When   the   flow   field   is   straight   enough   for   the application, we stop.

Generally, for any time-differentiable process , can be estimated by solving:

<!-- image -->

In rectified flow, by injecting strong priors that intermediate trajectories are straight, it can achieve both theoretical relevance for optimal transport and computational efficiency, as ODEs with straight paths can be simulated precisely without time discretization.

Specifically,   rectified   flow   seeks   to   match   an   ODE   with   the marginal   distributions   of   the linear   interpolation between points from distributions and . Given observations and , the canonical linear interpolation yields a trivial case , which cannot be causally simulated without . To address this, is   "projected"   into   a   space   of   causally   simulatable   ODEs,   by minimizing the least squares loss with respect to the direction :

<!-- image -->

Transport by rectified flow [47]

<!-- image -->

The data pair can be any coupling of and , typically independent (i.e., ) obtained by randomly combining observations from and . This process ensures that the trajectories closely mirror the density map of trajectories but reroute at intersections to ensure causality. This rectifying   process   is   also   known   as   Flow   Matching, [49] Stochastic   Interpolation, [50] and   alpha(de)blending. [51]

A distinctive aspect of rectified flow is its capability for " reflow ", which straightens the trajectory of ODE paths. Denote the rectified flow induced from as . Recursively applying this operator generates a series of rectified flows .   This   "reflow" process not only reduces transport costs but also straightens the paths of rectified flows, making paths straighter with increasing .

Rectified flow includes a nonlinear extension where linear interpolation is replaced with any time-differentiable curve that connects and ,   given   by . This framework encompasses DDIM and probability flow ODEs as special cases, with particular choices of and . However, in the case where the path of is not straight, the   reflow   process   no   longer   ensures   a

The reflow process [47]

<!-- image -->

reduction in convex transport costs, and also no longer straighten the paths of . [47]

See [52] for a tutorial on flow matching, with animations.

## Choice of architecture

## Diffusion model

For generating images by DDPM, we need a neural network that takes a time   and a noisy image , and predicts a noise from it. Since predicting the noise is the same as predicting the denoised image, then subtracting it from , denoising architectures tend to work well. For example, the U-Net, which was found to be good for denoising images, is often used for denoising diffusion models that generate images. [53]

For DDPM, the underlying architecture ("backbone") does not have to be a U-Net. It just has to predict the noise somehow. For example, the diffusion transformer (DiT) uses a Transformer to predict the mean and diagonal covariance of the noise, given the textual conditioning and the partially denoised image. It is the same as standard UNet-based denoising diffusion model, with a Transformer replacing the U-Net. [54] Mixture of experts-Transformer can also be applied. [55]

Architecture of Stable Diffusion

<!-- image -->

The denoising process used by Stable Diffusion

<!-- image -->

DDPM can be used to model general data distributions, not just  natural-looking   images.   For   example,   Human   Motion   Diffusion [56] models   human   motion trajectory by DDPM. Each human motion trajectory is a sequence of poses, represented by either joint rotations or positions. It uses a Transformer network to generate a less noisy trajectory out of a noisy one.

## Conditioning

The base diffusion model can only generate unconditionally from the whole distribution. For example, a diffusion model learned on ImageNet would generate images that look like a random image from ImageNet. To generate images from just one category, one would need to impose the condition, and then sample from the conditional distribution. Whatever condition one wants to impose, one needs to first

convert the conditioning into a vector of floating point numbers, then feed it into the underlying diffusion model neural network. However, one has freedom in choosing how to convert the conditioning into a vector.

Stable Diffusion, for example, imposes conditioning in the form of cross-attention mechanism, where the query is an intermediate representation of the image in the U-Net, and both key and value are the conditioning vectors. The conditioning can be selectively applied to only parts of an image, and new kinds of conditionings can be finetuned upon the base model, as used in ControlNet. [57]

As a particularly simple example, consider image inpainting. The conditions are , the reference image, and , the inpainting mask. The conditioning is imposed at each step of the backward diffusion process, by   first sampling , a noisy   version   of , then   replacing with , where means elementwise multiplication. [58] Another application of crossattention mechanism is prompt-to-prompt image editing. [59]

Conditioning is not limited to just generating images from a specific category, or according to a specific caption (as in text-to-image). For example, [56] demonstrated generating human motion, conditioned on an audio clip of human walking (allowing syncing motion to a soundtrack), or video of human running, or a text description of human motion, etc. For how conditional diffusion models are mathematically formulated, see a methodological summary in. [60]

## Upscaling

As generating an image takes a long time, one can try to generate a small image by a base diffusion model, then upscale it by other models. Upscaling can be done by GAN, [61] Transformer, [62] or signal processing methods like Lanczos resampling.

Diffusion models themselves can be used to perform upscaling. Cascading diffusion model stacks multiple diffusion models one after another, in the style of Progressive GAN. The lowest level is a standard diffusion model that generate 32x32 image, then the image would be upscaled by a diffusion model specifically trained for upscaling, and the process repeats. [53]

In more detail, the diffusion upscaler is trained as follows: [53]

- ▪ Sample , where is the high-resolution image, is the same image but scaled down to a low-resolution, and is the conditioning, which can be the caption of the image, the class of the image, etc.
- ▪ Sample two white noises , two time-steps . Compute the noisy versions of the high-

<!-- formula-not-decoded -->

- ▪ Train the denoising network to predict given . That is, apply gradient descent on on the L2 loss .

## Examples

This section collects some notable diffusion models, and briefly describes their architecture.

## OpenAI

The DALL-E series by OpenAI are text-conditional diffusion models of images.

The first version of DALL-E (2021) is not actually a diffusion model. Instead, it uses a Transformer architecture that autoregressively generates a sequence of tokens, which is then converted to an image by the decoder of a discrete VAE. Released with DALL-E was the CLIP classifier, which was used by DALL-E to rank generated images according to how close the image fits the text.

GLIDE (2022-03) [63] is a 3.5-billion diffusion model, and a small version was released publicly. [6] Soon after, DALL-E 2 was released (2022-04). [64] DALL-E 2 is a 3.5-billion cascaded diffusion model that generates images from text by "inverting the CLIP image encoder", the technique which they termed "unCLIP".

The unCLIP method contains 4 models: a CLIP image encoder, a CLIP text encoder, an image decoder, and a "prior" model (which can be a diffusion model, or an autoregressive model). During training, the prior model is trained to convert CLIP image encodings to CLIP text encodings. The image decoder is trained to convert CLIP image encodings back to images. During inference, a text is converted by the CLIP text encoder to a vector, then it is converted by the prior model to an image encoding, then it is converted by the image decoder to an image.

Sora (2024-02) is a diffusion Transformer model (DiT).

## Stability AI

Stable Diffusion (2022-08), released by Stability AI, consists of a denoising latent diffusion model (860 million parameters), a VAE, and a text encoder. The denoising network is a U-Net, with cross-attention blocks to allow for conditional image generation. [65][26]

Stable Diffusion 3 (2024-03) [66] changed the latent diffusion model from the UNet to a Transformer model, and so it is a DiT. It uses rectified flow.

Stable Video 4D (2024-07) [67] is a latent diffusion model for videos of 3D objects.

## Google

Imagen (2022) [68][69] uses a T5-XXL language model to encode the input text into an embedding vector. It is a cascaded diffusion model with three sub-models. The first step denoises a white noise to a 64×64 image, conditional on the embedding vector of the text. This model has 2B parameters. The second step upscales the image by 64×64 → 256×256, conditional on embedding. This model has 650M parameters. The third step is similar, upscaling by 256×256 → 1024×1024. This model has 400M parameters. The three denoising networks are all U-Nets.

Muse (2023-01) [70] is not a diffusion model, but an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens.

Imagen 2 (2023-12) is also diffusion-based. It can generate images based on a prompt that mixes images and text. No further information available. [71] Imagen   3   (2024-05)   is   too.   No   further   information available. [72]

Veo (2024) generates videos by latent diffusion. The diffusion is conditioned on a vector that encodes both a text prompt and an image prompt. [73]

## Meta

Make-A-Video (2022) is a text-to-video diffusion model. [74][75]

CM3leon (2023) is not a diffusion model, but an autoregressive causally masked Transformer, with mostly the same architecture as LLaMa-2. [76][77]

Transfusion   (2024)   is   a   Transformer   that   combines autoregressive   text   generation   and   denoising   diffusion. Specifically,   it   generates   text   autoregressively   (with   causal masking), and generates images by denoising multiple times over image tokens (with all-to-all attention). [78]

Transfusion architectural diagram

<!-- image -->

Movie Gen (2024) is a series of Diffusion Transformers operating on latent space and by flow matching. [79]

## See also

- ▪ Diffusion process
- ▪ Markov chain
- ▪ Variational inference
- ▪ Variational autoencoder

## Further reading

- ▪ Review papers
- ▪ Yang, Ling (2024-09-06), YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy (https://githu b.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy), retrieved 2024-09-06
- ▪ Yang, Ling; Zhang, Zhilong; Song, Yang; Hong, Shenda; Xu, Runsheng; Zhao, Yue; Zhang, Wentao; Cui, Bin; Yang, Ming-Hsuan (2023-11-09). "Diffusion Models: A Comprehensive Survey of Methods and Applications" (https://dl.acm.org/doi/abs/10.1145/3626235). ACM Comput. Surv . 56 (4): 105:1-105:39. arXiv:2209.00796 (https://arxiv.org/abs/2209.00796). doi:10.1145/3626235 (https://doi.org/10.1145%2F3626235). ISSN 0360-0300 (https://search.worldcat.org/issn/0360-030 0).
- ▪ Austin, Jacob; Johnson, Daniel D.; Ho, Jonathan; Tarlow, Daniel; Rianne van den Berg (2021). "Structured Denoising Diffusion Models in Discrete State-Spaces". arXiv:2107.03006 (https://arxi v.org/abs/2107.03006) [cs.LG (https://arxiv.org/archive/cs.LG)].
- ▪ Croitoru, Florinel-Alin; Hondru, Vlad; Ionescu, Radu Tudor; Shah, Mubarak (2023-09-01). "Diffusion Models in Vision: A Survey" (https://ieeexplore.ieee.org/document/10081412). IEEE Transactions on Pattern Analysis and Machine Intelligence . 45 (9): 10850-10869. arXiv:2209.04747 (https://arxiv.org/abs/2209.04747). doi:10.1109/TPAMI.2023.3261988 (https://do i.org/10.1109%2FTPAMI.2023.3261988). ISSN 0162-8828 (https://search.worldcat.org/issn/016 2-8828). PMID 37030794 (https://pubmed.ncbi.nlm.nih.gov/37030794).
- ▪ Mathematical details omitted in the article.

- ▪ "Power of Diffusion Models" (https://astralord.github.io/posts/power-of-diffusion-models/). AstraBlog . 2022-09-25. Retrieved 2023-09-25.
- ▪ Luo, Calvin (2022-08-25). "Understanding Diffusion Models: A Unified Perspective". arXiv:2208.11970 (https://arxiv.org/abs/2208.11970) [cs.LG (https://arxiv.org/archive/cs.LG)].
- ▪ Weng, Lilian (2021-07-11). "What are Diffusion Models?" (https://lilianweng.github.io/posts/2021-0 7-11-diffusion-models/). lilianweng.github.io . Retrieved 2023-09-25.

## ▪ Tutorials

- ▪ Nakkiran, Preetum; Bradley, Arwen; Zhou, Hattie; Advani, Madhu (2024). "Step-by-Step Diffusion: An Elementary Tutorial". arXiv:2406.08929 (https://arxiv.org/abs/2406.08929) [cs.LG (https://arxi v.org/archive/cs.LG)].
- ▪ "Guidance: a cheat code for diffusion models" (https://benanne.github.io/2022/05/26/guidance.htm l). 26 May 2022. Overview of classifier guidance and classifier-free guidance, light on mathematical details.

## References

- 1. Chang, Ziyi; Koulieris, George Alex; Shum, Hubert P. H. (2023). "On the Design Fundamentals of Diffusion Models: A Survey". arXiv:2306.04542 (https://arxiv.org/abs/2306.04542) [cs.LG (https://arxi v.org/archive/cs.LG)].
- 2. Song, Yang; Sohl-Dickstein, Jascha; Kingma, Diederik P.; Kumar, Abhishek; Ermon, Stefano; Poole, Ben (2021-02-10). "Score-Based Generative Modeling through Stochastic Differential Equations". arXiv:2011.13456 (https://arxiv.org/abs/2011.13456) [cs.LG (https://arxiv.org/archive/cs.LG)].
- 3. Croitoru, Florinel-Alin; Hondru, Vlad; Ionescu, Radu Tudor; Shah, Mubarak (2023). "Diffusion Models in Vision: A Survey". IEEE Transactions on Pattern Analysis and Machine Intelligence . 45 (9): 1085010869. arXiv:2209.04747 (https://arxiv.org/abs/2209.04747). doi:10.1109/TPAMI.2023.3261988 (http s://doi.org/10.1109%2FTPAMI.2023.3261988). PMID 37030794 (https://pubmed.ncbi.nlm.nih.gov/370 30794). S2CID 252199918 (https://api.semanticscholar.org/CorpusID:252199918).
- 4. Ho, Jonathan; Jain, Ajay; Abbeel, Pieter (2020). "Denoising Diffusion Probabilistic Models" (https://pro ceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html). Advances in Neural Information Processing Systems . 33 . Curran Associates, Inc.: 6840-6851.
- 5. Gu, Shuyang; Chen, Dong; Bao, Jianmin; Wen, Fang; Zhang, Bo; Chen, Dongdong; Yuan, Lu; Guo, Baining (2021). "Vector Quantized Diffusion Model for Text-to-Image Synthesis". arXiv:2111.14822 (htt ps://arxiv.org/abs/2111.14822) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 6. GLIDE (https://github.com/openai/glide-text2im), OpenAI, 2023-09-22, retrieved 2023-09-24
- 7. Nie, Shen; Zhu, Fengqi; Du, Chao; Pang, Tianyu; Liu, Qian; Zeng, Guangtao; Lin, Min; Li, Chongxuan (2024). "Scaling up Masked Diffusion Models on Text". arXiv:2410.18514 (https://arxiv.org/abs/2410.1 8514) [cs.AI (https://arxiv.org/archive/cs.AI)].
- 8. Li, Yifan; Zhou, Kun; Zhao, Wayne Xin; Wen, Ji-Rong (August 2023). "Diffusion Models for Nonautoregressive Text Generation: A Survey" (https://dx.doi.org/10.24963/ijcai.2023/750). Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . California: International Joint Conferences on Artificial Intelligence Organization. pp. 6692-6701. arXiv:2303.06574 (https://ar xiv.org/abs/2303.06574). doi:10.24963/ijcai.2023/750 (https://doi.org/10.24963%2Fijcai.2023%2F750) . ISBN 978-1-956792-03-4.
- 9. Han, Xiaochuang; Kumar, Sachin; Tsvetkov, Yulia (2023). "SSD-LM: Semi-autoregressive Simplexbased Diffusion Language Model for Text Generation and Modular Control" (https://dx.doi.org/10.1865 3/v1/2023.acl-long.647). Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Stroudsburg, PA, USA: Association for Computational Linguistics: 11575-11596. arXiv:2210.17432 (https://arxiv.org/abs/2210.17432). doi:10.18653/ v1/2023.acl-long.647 (https://doi.org/10.18653%2Fv1%2F2023.acl-long.647).

- 10. Xu, Weijie; Hu, Wenxiang; Wu, Fanyou; Sengamedu, Srinivasan (2023). "DeTiME: DiffusionEnhanced Topic Modeling using Encoder-decoder based LLM" (https://dx.doi.org/10.18653/v1/2023.fi ndings-emnlp.606). Findings of the Association for Computational Linguistics: EMNLP 2023 . Stroudsburg, PA, USA: Association for Computational Linguistics: 9040-9057. arXiv:2310.15296 (http s://arxiv.org/abs/2310.15296). doi:10.18653/v1/2023.findings-emnlp.606 (https://doi.org/10.18653%2F v1%2F2023.findings-emnlp.606).
- 11. Zhang, Haopeng; Liu, Xiao; Zhang, Jiawei (2023). "DiffuSum: Generation Enhanced Extractive Summarization with Diffusion" (https://dx.doi.org/10.18653/v1/2023.findings-acl.828). Findings of the Association for Computational Linguistics: ACL 2023 . Stroudsburg, PA, USA: Association for Computational Linguistics: 13089-13100. arXiv:2305.01735 (https://arxiv.org/abs/2305.01735). doi:10.18653/v1/2023.findings-acl.828 (https://doi.org/10.18653%2Fv1%2F2023.findings-acl.828).
- 12. Yang, Dongchao; Yu, Jianwei; Wang, Helin; Wang, Wen; Weng, Chao; Zou, Yuexian; Yu, Dong (2023). "Diffsound: Discrete Diffusion Model for Text-to-Sound Generation" (https://dx.doi.org/10.1109/ taslp.2023.3268730). IEEE/ACM Transactions on Audio, Speech, and Language Processing . 31 : 1720-1733. arXiv:2207.09983 (https://arxiv.org/abs/2207.09983). doi:10.1109/taslp.2023.3268730 (htt ps://doi.org/10.1109%2Ftaslp.2023.3268730). ISSN 2329-9290 (https://search.worldcat.org/issn/232 9-9290).
- 13. Janner, Michael; Du, Yilun; Tenenbaum, Joshua B.; Levine, Sergey (2022-12-20). "Planning with Diffusion for Flexible Behavior Synthesis". arXiv:2205.09991 (https://arxiv.org/abs/2205.09991) [cs.LG (https://arxiv.org/archive/cs.LG)].
- 14. Chi, Cheng; Xu, Zhenjia; Feng, Siyuan; Cousineau, Eric; Du, Yilun; Burchfiel, Benjamin; Tedrake, Russ; Song, Shuran (2024-03-14). "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion". arXiv:2303.04137 (https://arxiv.org/abs/2303.04137) [cs.RO (https://arxiv.org/archive/cs.RO)].
- 15. Sohl-Dickstein, Jascha; Weiss, Eric; Maheswaranathan, Niru; Ganguli, Surya (2015-06-01). "Deep Unsupervised Learning using Nonequilibrium Thermodynamics" (http://proceedings.mlr.press/v37/soh l-dickstein15.pdf) (PDF). Proceedings of the 32nd International Conference on Machine Learning . 37 . PMLR: 2256-2265. arXiv:1503.03585 (https://arxiv.org/abs/1503.03585).
- 16. Ho, Jonathan (Jun 20, 2020), hojonathanho/diffusion (https://github.com/hojonathanho/diffusion), retrieved 2024-09-07
- 17. Weng, Lilian (2021-07-11). "What are Diffusion Models?" (https://lilianweng.github.io/posts/2021-07-1 1-diffusion-models/). lilianweng.github.io . Retrieved 2023-09-24.
- 18. "Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song" (https://yang-son g.net/blog/2021/score/). yang-song.net . Retrieved 2023-09-24.
- 19. Song, Yang; Ermon, Stefano (2019). "Generative Modeling by Estimating Gradients of the Data Distribution" (https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93Abstract.html). Advances in Neural Information Processing Systems . 32 . Curran Associates, Inc. arXiv:1907.05600 (https://arxiv.org/abs/1907.05600).
- 20. Song, Yang; Sohl-Dickstein, Jascha; Kingma, Diederik P.; Kumar, Abhishek; Ermon, Stefano; Poole, Ben (2021-02-10). "Score-Based Generative Modeling through Stochastic Differential Equations". arXiv:2011.13456 (https://arxiv.org/abs/2011.13456) [cs.LG (https://arxiv.org/archive/cs.LG)].
- 21. ermongroup/ncsn (https://github.com/ermongroup/ncsn), ermongroup, 2019, retrieved 2024-09-07
- 22. "Sliced Score Matching: A Scalable Approach to Density and Score Estimation | Yang Song" (https://y ang-song.net/blog/2019/ssm/). yang-song.net . Retrieved 2023-09-24.
- 23. Anderson, Brian D.O. (May 1982). "Reverse-time diffusion equation models" (https://dx.doi.org/10.101 6/0304-4149(82)90051-5). Stochastic Processes and Their Applications . 12 (3): 313-326. doi:10.1016/0304-4149(82)90051-5 (https://doi.org/10.1016%2F0304-4149%2882%2990051-5). ISSN 0304-4149 (https://search.worldcat.org/issn/0304-4149).
- 24. Luo, Calvin (2022). "Understanding Diffusion Models: A Unified Perspective". arXiv:2208.11970v1 (htt ps://arxiv.org/abs/2208.11970v1) [cs.LG (https://arxiv.org/archive/cs.LG)].
- 25. Song, Jiaming; Meng, Chenlin; Ermon, Stefano (3 Oct 2023). "Denoising Diffusion Implicit Models". arXiv:2010.02502 (https://arxiv.org/abs/2010.02502) [cs.LG (https://arxiv.org/archive/cs.LG)].

- 26. Rombach, Robin; Blattmann, Andreas; Lorenz, Dominik; Esser, Patrick; Ommer, Björn (13 April 2022). "High-Resolution Image Synthesis With Latent Diffusion Models". arXiv:2112.10752 (https://arxiv.org/a bs/2112.10752) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 27. Nichol, Alexander Quinn; Dhariwal, Prafulla (2021-07-01). "Improved Denoising Diffusion Probabilistic Models" (https://proceedings.mlr.press/v139/nichol21a.html). Proceedings of the 38th International Conference on Machine Learning . PMLR: 8162-8171.
- 28. Salimans, Tim; Ho, Jonathan (2021-10-06). Progressive Distillation for Fast Sampling of Diffusion Models (https://openreview.net/forum?id=TIdIXIpzhoI). The Tenth International Conference on Learning Representations (ICLR 2022).
- 29. Lin, Shanchuan; Liu, Bingchen; Li, Jiashi; Yang, Xiao (2024). Common Diffusion Noise Schedules and Sample Steps Are Flawed (https://openaccess.thecvf.com/content/WACV2024/html/Lin\_Common\_Diff usion\_Noise\_Schedules\_and\_Sample\_Steps\_Are\_Flawed\_WACV\_2024\_paper.html). IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp. 5404-5411.
- 30. Dhariwal, Prafulla; Nichol, Alex (2021-06-01). "Diffusion Models Beat GANs on Image Synthesis". arXiv:2105.05233 (https://arxiv.org/abs/2105.05233) [cs.LG (https://arxiv.org/archive/cs.LG)].
- 31. Ho, Jonathan; Salimans, Tim (2022-07-25). "Classifier-Free Diffusion Guidance". arXiv:2207.12598 (h ttps://arxiv.org/abs/2207.12598) [cs.LG (https://arxiv.org/archive/cs.LG)].
- 32. Chung, Hyungjin; Kim, Jeongsol; Park, Geon Yeong; Nam, Hyelin; Ye, Jong Chul (2024-06-12). "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models". arXiv:2406.08070 (http s://arxiv.org/abs/2406.08070) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 33. Sanchez, Guillaume; Fan, Honglu; Spangher, Alexander; Levi, Elad; Ammanamanchi, Pawan Sasanka; Biderman, Stella (2023-06-30). "Stay on topic with Classifier-Free Guidance". arXiv:2306.17806 (https://arxiv.org/abs/2306.17806) [cs.CL (https://arxiv.org/archive/cs.CL)].
- 34. Armandpour, Mohammadreza; Sadeghian, Ali; Zheng, Huangjie; Sadeghian, Amir; Zhou, Mingyuan (2023-04-26). "Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond". arXiv:2304.04968 (https://arxiv.org/abs/2304.04968) [cs.CV (https://arxi v.org/archive/cs.CV)].
- 35. Yang, Ling; Zhang, Zhilong; Song, Yang; Hong, Shenda; Xu, Runsheng; Zhao, Yue; Zhang, Wentao; Cui, Bin; Yang, Ming-Hsuan (2022). "Diffusion Models: A Comprehensive Survey of Methods and Applications". arXiv:2206.00364 (https://arxiv.org/abs/2206.00364) [cs.CV (https://arxiv.org/archive/c s.CV)].
- 36. Shi, Jiaxin; Han, Kehang; Wang, Zhe; Doucet, Arnaud; Titsias, Michalis K. (2024). "Simplified and Generalized Masked Diffusion for Discrete Data". arXiv:2406.04329 (https://arxiv.org/abs/2406.04329) [cs.LG (https://arxiv.org/archive/cs.LG)].
- 37. Karras, Tero; Aittala, Miika; Aila, Timo; Laine, Samuli (2022). "Elucidating the Design Space of Diffusion-Based Generative Models". arXiv:2206.00364v2 (https://arxiv.org/abs/2206.00364v2) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 38. Cao, Hanqun; Tan, Cheng; Gao, Zhangyang; Xu, Yilun; Chen, Guangyong; Heng, Pheng-Ann; Li, Stan Z. (July 2024). "A Survey on Generative Diffusion Models" (https://ieeexplore.ieee.org/documen t/10419041). IEEE Transactions on Knowledge and Data Engineering . 36 (7): 2814-2830. doi:10.1109/TKDE.2024.3361474 (https://doi.org/10.1109%2FTKDE.2024.3361474). ISSN 1041-4347 (https://search.worldcat.org/issn/1041-4347).
- 39. Xu, Yilun; Liu, Ziming; Tian, Yonglong; Tong, Shangyuan; Tegmark, Max; Jaakkola, Tommi (2023-07-03). "PFGM++: Unlocking the Potential of Physics-Inspired Generative Models" (https://proc eedings.mlr.press/v202/xu23m.html). Proceedings of the 40th International Conference on Machine Learning . PMLR: 38566-38591. arXiv:2302.04265 (https://arxiv.org/abs/2302.04265).
- 40. Song, Yang; Dhariwal, Prafulla; Chen, Mark; Sutskever, Ilya (2023-07-03). "Consistency Models" (http s://proceedings.mlr.press/v202/song23a). Proceedings of the 40th International Conference on Machine Learning . PMLR: 32211-32252.

- 41. Dockhorn, Tim; Vahdat, Arash; Kreis, Karsten (2021-10-06). "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion". arXiv:2112.07068 (https://arxiv.org/abs/2112.07068) [stat.ML (https://arxiv.org/archive/stat.ML)].
- 42. Liu, Ziming; Luo, Di; Xu, Yilun; Jaakkola, Tommi; Tegmark, Max (2023-04-05). "GenPhys: From Physical Processes to Generative Models". arXiv:2304.02637 (https://arxiv.org/abs/2304.02637) [cs.LG (https://arxiv.org/archive/cs.LG)].
- 43. Bansal, Arpit; Borgnia, Eitan; Chu, Hong-Min; Li, Jie; Kazemi, Hamid; Huang, Furong; Goldblum, Micah; Geiping, Jonas; Goldstein, Tom (2023-12-15). "Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise" (https://proceedings.neurips.cc/paper\_files/paper/2023/hash/80fe51a7d8d 0c73ff7439c2a2554ed53-Abstract-Conference.html). Advances in Neural Information Processing Systems . 36 : 41259-41282. arXiv:2208.09392 (https://arxiv.org/abs/2208.09392).
- 44. Gulrajani, Ishaan; Hashimoto, Tatsunori B. (2023-12-15). "Likelihood-Based Diffusion Language Models" (https://proceedings.neurips.cc/paper\_files/paper/2023/hash/35b5c175e139bff5f22a5361270 fce87-Abstract-Conference.html). Advances in Neural Information Processing Systems . 36 : 1669316715. arXiv:2305.18619 (https://arxiv.org/abs/2305.18619).
- 45. Lou, Aaron; Meng, Chenlin; Ermon, Stefano (2024-06-06). "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution". arXiv:2310.16834 (https://arxiv.org/abs/2310.16834) [stat.ML (http s://arxiv.org/archive/stat.ML)].
- 46. Tong, Alexander; Fatras, Kilian; Malkin, Nikolay; Huguet, Guillaume; Zhang, Yanlei; Rector-Brooks, Jarrid; Wolf, Guy; Bengio, Yoshua (2023-11-08). "Improving and generalizing flow-based generative models with minibatch optimal transport" (https://openreview.net/forum?id=CD9Snc73AW). Transactions on Machine Learning Research . arXiv:2302.00482 (https://arxiv.org/abs/2302.00482). ISSN 2835-8856 (https://search.worldcat.org/issn/2835-8856).
- 47. Liu, Xingchao; Gong, Chengyue; Liu, Qiang (2022-09-07). "Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow". arXiv:2209.03003 (https://arxiv.org/abs/2209.03003) [cs.LG (https://arxiv.org/archive/cs.LG)].
- 48. Liu, Qiang (2022-09-29). "Rectified Flow: A Marginal Preserving Approach to Optimal Transport". arXiv:2209.14577 (https://arxiv.org/abs/2209.14577) [stat.ML (https://arxiv.org/archive/stat.ML)].
- 49. Lipman, Yaron; Chen, Ricky T. Q.; Ben-Hamu, Heli; Nickel, Maximilian; Le, Matt (2023-02-08). "Flow Matching for Generative Modeling". arXiv:2210.02747 (https://arxiv.org/abs/2210.02747) [cs.LG (http s://arxiv.org/archive/cs.LG)].
- 50. Albergo, Michael S.; Vanden-Eijnden, Eric (2023-03-09). "Building Normalizing Flows with Stochastic Interpolants". arXiv:2209.15571 (https://arxiv.org/abs/2209.15571) [cs.LG (https://arxiv.org/archive/c s.LG)].
- 51. Heitz, Eric; Belcour, Laurent; Chambon, Thomas (2023-05-05). "Iterative α -(de)Blending: a Minimalist Deterministic Diffusion Model". arXiv:2305.03486 (https://arxiv.org/abs/2305.03486) [cs.GR (https://ar xiv.org/archive/cs.GR)].
- 52. "An introduction to Flow Matching · Cambridge MLG Blog" (https://mlg.eng.cam.ac.uk/blog/2024/01/2 0/flow-matching.html). mlg.eng.cam.ac.uk . Retrieved 2024-08-20.
- 53. Ho, Jonathan; Saharia, Chitwan; Chan, William; Fleet, David J.; Norouzi, Mohammad; Salimans, Tim (2022-01-01). "Cascaded diffusion models for high fidelity image generation" (https://dl.acm.org/doi/ab s/10.5555/3586589.3586636). The Journal of Machine Learning Research . 23 (1): 47:2249-47:2281. arXiv:2106.15282 (https://arxiv.org/abs/2106.15282). ISSN 1532-4435 (https://search.worldcat.org/iss n/1532-4435).
- 54. Peebles, William; Xie, Saining (March 2023). "Scalable Diffusion Models with Transformers". arXiv:2212.09748v2 (https://arxiv.org/abs/2212.09748v2) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 55. Fei, Zhengcong; Fan, Mingyuan; Yu, Changqian; Li, Debang; Huang, Junshi (2024-07-16). "Scaling Diffusion Transformers to 16 Billion Parameters". arXiv:2407.11633 (https://arxiv.org/abs/2407.11633) [cs.CV (https://arxiv.org/archive/cs.CV)].

- 56. Tevet, Guy; Raab, Sigal; Gordon, Brian; Shafir, Yonatan; Cohen-Or, Daniel; Bermano, Amit H. (2022). "Human Motion Diffusion Model". arXiv:2209.14916 (https://arxiv.org/abs/2209.14916) [cs.CV (https:// arxiv.org/archive/cs.CV)].
- 57. Zhang, Lvmin; Rao, Anyi; Agrawala, Maneesh (2023). "Adding Conditional Control to Text-to-Image Diffusion Models". arXiv:2302.05543 (https://arxiv.org/abs/2302.05543) [cs.CV (https://arxiv.org/archiv e/cs.CV)].
- 58. Lugmayr, Andreas; Danelljan, Martin; Romero, Andres; Yu, Fisher; Timofte, Radu; Van Gool, Luc (2022). "RePaint: Inpainting Using Denoising Diffusion Probabilistic Models". arXiv:2201.09865v4 (htt ps://arxiv.org/abs/2201.09865v4) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 59. Hertz, Amir; Mokady, Ron; Tenenbaum, Jay; Aberman, Kfir; Pritch, Yael; Cohen-Or, Daniel (2022-08-02). "Prompt-to-Prompt Image Editing with Cross Attention Control". arXiv:2208.01626 (http s://arxiv.org/abs/2208.01626) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 60. Zhao, Zheng; Luo, Ziwei; Sjölund, Jens; Schön, Thomas B. (2024). "Conditional sampling within generative diffusion models". arXiv:2409.09650 (https://arxiv.org/abs/2409.09650) [stat.ML (https://arx iv.org/archive/stat.ML)].
- 61. Wang, Xintao; Xie, Liangbin; Dong, Chao; Shan, Ying (2021). "Real-ESRGAN: Training Real-World Blind Super-Resolution With Pure Synthetic Data" (https://openaccess.thecvf.com/content/ICCV2021 W/AIM/papers/Wang\_Real-ESRGAN\_Training\_Real-World\_Blind\_Super-Resolution\_With\_Pure\_Synt hetic\_Data\_ICCVW\_2021\_paper.pdf) (PDF). Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, 2021 . International Conference on Computer Vision. pp. 1905-1914. arXiv:2107.10833 (https://arxiv.org/abs/2107.10833).
- 62. Liang, Jingyun; Cao, Jiezhang; Sun, Guolei; Zhang, Kai; Van Gool, Luc; Timofte, Radu (2021). "SwinIR: Image Restoration Using Swin Transformer" (https://openaccess.thecvf.com/content/ICCV20 21W/AIM/papers/Liang\_SwinIR\_Image\_Restoration\_Using\_Swin\_Transformer\_ICCVW\_2021\_pape r.pdf) (PDF). Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops . International Conference on Computer Vision, 2021. pp. 1833-1844. arXiv:2108.10257v1 (https://arxiv.org/abs/2108.10257v1).
- 63. Nichol, Alex; Dhariwal, Prafulla; Ramesh, Aditya; Shyam, Pranav; Mishkin, Pamela; McGrew, Bob; Sutskever, Ilya; Chen, Mark (2022-03-08). "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models". arXiv:2112.10741 (https://arxiv.org/abs/2112.10741) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 64. Ramesh, Aditya; Dhariwal, Prafulla; Nichol, Alex; Chu, Casey; Chen, Mark (2022-04-12). "Hierarchical Text-Conditional Image Generation with CLIP Latents". arXiv:2204.06125 (https://arxiv.org/abs/2204.0 6125) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 65. Alammar, Jay. "The Illustrated Stable Diffusion" (https://jalammar.github.io/illustrated-stable-diffusion/). jalammar.github.io . Retrieved 2022-10-31.
- 66. Esser, Patrick; Kulal, Sumith; Blattmann, Andreas; Entezari, Rahim; Müller, Jonas; Saini, Harry; Levi, Yam; Lorenz, Dominik; Sauer, Axel (2024-03-05). "Scaling Rectified Flow Transformers for HighResolution Image Synthesis". arXiv:2403.03206 (https://arxiv.org/abs/2403.03206) [cs.CV (https://arxi v.org/archive/cs.CV)].
- 67. Xie, Yiming; Yao, Chun-Han; Voleti, Vikram; Jiang, Huaizu; Jampani, Varun (2024-07-24). "SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency". arXiv:2407.17470 (ht tps://arxiv.org/abs/2407.17470) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 68. "Imagen: Text-to-Image Diffusion Models" (https://imagen.research.google/). imagen.research.google . Retrieved 2024-04-04.
- 69. Saharia, Chitwan; Chan, William; Saxena, Saurabh; Li, Lala; Whang, Jay; Denton, Emily L.; Ghasemipour, Kamyar; Gontijo Lopes, Raphael; Karagol Ayan, Burcu; Salimans, Tim; Ho, Jonathan; Fleet, David J.; Norouzi, Mohammad (2022-12-06). "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding" (https://proceedings.neurips.cc/paper\_files/paper/2022/hash/ec7 95aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html). Advances in Neural Information Processing Systems . 35 : 36479-36494. arXiv:2205.11487 (https://arxiv.org/abs/2205.11487).

- 70. Chang, Huiwen; Zhang, Han; Barber, Jarred; Maschinot, A. J.; Lezama, Jose; Jiang, Lu; Yang, MingHsuan; Murphy, Kevin; Freeman, William T. (2023-01-02). "Muse: Text-To-Image Generation via Masked Generative Transformers". arXiv:2301.00704 (https://arxiv.org/abs/2301.00704) [cs.CV (http s://arxiv.org/archive/cs.CV)].
- 71. "Imagen 2 - our most advanced text-to-image technology" (https://deepmind.google/technologies/ima gen-2/). Google DeepMind . Retrieved 2024-04-04.
- 72. Imagen-Team-Google; Baldridge, Jason; Bauer, Jakob; Bhutani, Mukul; Brichtova, Nicole; Bunner, Andrew; Castrejon, Lluis; Chan, Kelvin; Chen, Yichang (2024-12-13), Imagen 3 , arXiv:2408.07009 (htt ps://arxiv.org/abs/2408.07009) {{citation}} : |last1= has generic name (help)
- 73. "Veo" (https://deepmind.google/technologies/veo/). Google DeepMind . 2024-05-14. Retrieved 2024-05-17.
- 74. "Introducing Make-A-Video: An AI system that generates videos from text" (https://ai.meta.com/blog/g enerative-ai-text-to-video/). ai.meta.com . Retrieved 2024-09-20.
- 75. Singer, Uriel; Polyak, Adam; Hayes, Thomas; Yin, Xi; An, Jie; Zhang, Songyang; Hu, Qiyuan; Yang, Harry; Ashual, Oron (2022-09-29). "Make-A-Video: Text-to-Video Generation without Text-Video Data". arXiv:2209.14792 (https://arxiv.org/abs/2209.14792) [cs.CV (https://arxiv.org/archive/cs.CV)].
- 76. "Introducing CM3leon, a more efficient, state-of-the-art generative model for text and images" (https:// ai.meta.com/blog/generative-ai-text-images-cm3leon/). ai.meta.com . Retrieved 2024-09-20.
- 77. Chameleon Team (2024-05-16). "Chameleon: Mixed-Modal Early-Fusion Foundation Models". arXiv:2405.09818 (https://arxiv.org/abs/2405.09818) [cs.CL (https://arxiv.org/archive/cs.CL)].
- 78. Zhou, Chunting; Yu, Lili; Babu, Arun; Tirumala, Kushal; Yasunaga, Michihiro; Shamis, Leonid; Kahn, Jacob; Ma, Xuezhe; Zettlemoyer, Luke (2024-08-20). "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model". arXiv:2408.11039 (https://arxiv.org/abs/2408.11039) [cs.AI (https://arxiv.org/archive/cs.AI)].
- 79. Movie Gen: A Cast of Media Foundation Models (https://ai.meta.com/static-resource/movie-gen-resea rch-paper) , The Movie Gen team @ Meta, October 4, 2024.

Retrieved from "https://en.wikipedia.org/w/index.php?title=Diffusion\_model&amp;oldid=1285837703"