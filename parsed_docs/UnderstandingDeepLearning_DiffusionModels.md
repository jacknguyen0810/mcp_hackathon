## Chapter 18

## Diusion models

Chapter 15 described generative adversarial models, which produce plausible-looking samples but do not dene a probability distribution over the data. Chapter 16 discussed normalizing ows. These do dene such a probability distribution but must place architectural constraints on the network; each layer must be invertible, and the determinant of its Jacobian must be easy to calculate. Chapter 17 introduced variational autoencoders, which also have a solid probabilistic foundation but where the computation of the likelihood is intractable and must be approximated by a lower bound.

This chapter introduces diusion models. Like normalizing ows, these are probabilistic models that dene a nonlinear mapping from latent variables to the observed data where both quantities have the same dimension. Like variational autoencoders, they approximate the data likelihood using a lower bound based on an encoder that maps to the latent variable. However, in diusion models, this encoder is predetermined; the goal is to learn a decoder that is the inverse of this process and can be used to produce samples. Diusion models are easy to train and can produce very high-quality samples that exceed the realism of those produced by GANs. The reader should be familiar with variational autoencoders (chapter 17) before reading this chapter.

## 18.1 Overview

A diusion model consists of an encoder and a decoder . The encoder takes a data sample x and maps it through a series of intermediate latent variables z 1    z T . The decoder reverses this process; it starts with z T and maps back through z T -1 ,    , z 1 until it nally (re-)creates a data point x . In both encoder and decoder, the mappings are stochastic rather than deterministic.

In the decoder, a series of networks are trained to map backward between each

The encoder is prespecied; it gradually blends the input with samples of white noise (gure 18.1). With enough steps, the conditional distribution q ( z T  x ) and marginal distribution q ( z T ) of the nal latent variable both become the standard normal distribution. Since this process is prespecied, all the learned parameters are in the decoder.

Figure 18.1 Diusion models. The encoder (forward, or diusion process) maps the input x through a series of latent variables z 1    z T . This process is prespecied and gradually mixes the data with noise until only noise remains. The decoder (reverse process) is learned and passes the data back through the latent variables, removing noise at each stage. After training, new examples are generated by sampling noise vectors z T and passing them through the decoder.

<!-- image -->

adjacent pair of latent variables z t and z t -1 . The loss function encourages each network to invert the corresponding encoder step. The result is that noise is gradually removed from the representation until a realistic-looking data example remains. To generate a new data example x , we draw a sample from q ( z T ) and pass it through the decoder.

In section 18.2, we consider the encoder in detail. Its properties are non-obvious but are critical for the learning algorithm. In section 18.3, we discuss the decoder. Section 18.4 derives the training algorithm, and section 18.5 reformulates it to be more practical. Section 18.6 discusses implementation details, including how to make the generation conditional on text prompts.

## 18.2 Encoder (forward process)

The diusion or forward process 1 (gure 18.2) maps a data example x through a series of intermediate variables z 1 , z 2 ,    , z T with the same size as x according to:

<!-- formula-not-decoded -->

where ϵ t is noise drawn from a standard normal distribution. The rst term attenuates the data plus any noise added so far, and the second adds more noise. The hyperparameters β t  [0 , 1] determine how quickly the noise is blended and are collectively known as the noise schedule . The forward process can equivalently be written as:

1 Note, this is the opposite nomenclature to normalizing ows, where the inverse mapping moves from the data to the latent variable, and the forward mapping moves back again.

## Problem 18.1

Figure 18.2 Forward process. a) We consider one-dimensional data x with T = 100 latent variables z , 1    , z 100 and β = 0 03  at all steps. Three values of x (gray, cyan, and orange) are initialized (top row). These are propagated through z , 1    , z 100 . At each step, the variable is updated by attenuating its value by √ 1 -β and adding noise with mean zero and variance β (equation 18.1). Accordingly, the three examples noisily propagate through the variables with a tendency to move toward zero. b) The conditional probabilities Pr z ( 1  x ) and Pr z ( t  z t -1 ) are normal distributions with a mean that is slightly closer to zero than the current point and a xed variance β t (equation 18.2).

<!-- image -->

<!-- formula-not-decoded -->

The joint distribution of all of the latent variables z 1 , z 2 ,    , z T given input x is:

This is a Markov chain because the probability of z t is determined entirely by the value of the immediately preceding variable z t -1 . With sucient steps T , all traces of the original data are removed, and q ( z T  x ) = q ( z T ) becomes a standard normal distribution. 2

<!-- formula-not-decoded -->

2 We use q ( z t  z t -1 ) rather than Pr ( z t  z t -1 ) to match the notation in the description of the VAE encoder in the previous chapter.

Figure 18.3 Diusion kernel. a) The point x ∗ =2 0  is propagated through the latent variables using equation 18.1 (ve paths shown in gray). The diusion kernel q z ( t  x ∗ ) is the probability distribution over variable z t given that we started from x ∗ . It can be computed in closed-form and is a normal distribution whose mean moves toward zero and whose variance increases as t increases. Heatmap shows q z ( t  x ∗ ) for each variable. Cyan lines show ± 2 standard deviations from the mean. b) The diusion kernel q z ( t  x ∗ ) is shown explicitly for t = 20 40 80 , , . In practice, the diusion kernel allows us to sample a latent variable z t corresponding to a given x ∗ without computing the intermediate variables z , 1    , z t -1 . When t becomes very large, the diusion kernel becomes a standard normal.

<!-- image -->

Figure 18.4 Marginal distributions. a) Given an initial density Pr x ( ) (top row), the diusion process gradually blurs the distribution as it passes through the latent variables z t and moves it toward a standard normal distribution. Each subsequent horizontal line of heatmap represents a marginal distribution q z ( t ) . b) The top graph shows the initial distribution Pr x ( ) . The other two graphs show the marginal distributions q z ( 20 ) and q z ( 60 ) , respectively.

<!-- image -->

## 18.2.1 Diusion kernel q ( z t  x )

To train the decoder to invert this process, we use multiple samples z t at time t for the same example x . However, generating these sequentially using equation 18.1 is timeconsuming when t is large. Fortunately, there is a closed-form expression for q ( z t  x ) , which allows us to directly draw samples z t given initial data point x without computing the intermediate variables z 1    z t -1 . This is known as the diusion kernel (gure 18.3).

To derive an expression for q ( z t  x ) , consider the rst two steps of the forward process:

<!-- formula-not-decoded -->

Substituting the rst equation into the second, we get:

<!-- formula-not-decoded -->

The last two terms are independent samples from mean-zero normal distributions with variances 1 -β 2 -(1 -β 2 )(1 -β 1 ) and β 2 , respectively. The mean of this sum is zero, and its variance is the sum of the component variances (see problem 18.2), so:

<!-- formula-not-decoded -->

If we continue this process by substituting this equation into the expression for z 3 and so on, we can show that:

where ϵ is also a sample from a standard normal distribution.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where α t =  t s =1 1 -β s . We can equivalently write this in probabilistic form:

For any starting data point x , variable z t is normally distributed with a known mean and variance. Consequently, if we don't care about the history of the evolution through the intermediate variables z 1    z t -1 , it is easy to generate samples from q ( z t  x ) .

## 18.2.2 Marginal distributions q ( z t )

The marginal distribution q ( z t ) is the probability of observing a value of z t given the distribution of possible starting points x and the possible diusion paths for each starting

## Problem 18.2

## Problem 18.3

point (gure 18.4). It can be computed by considering the joint distribution q ( x z , 1  t ) and marginalizing over all the variables except z t :

<!-- formula-not-decoded -->

where q ( z 1  t  x ) was dened in equation 18.3.

However, since we now have an expression for the diusion kernel q ( z t  x ) that 'skips' the intervening variables, we can equivalently write:

<!-- formula-not-decoded -->

Hence, if we repeatedly sample from the data distribution Pr ( x ) and superimpose the diusion kernel q ( z t  x ) on each sample, the result is the marginal distribution q ( z t ) (gure 18.4). However, the marginal distribution cannot be written in closed form because we don't know the original data distribution Pr ( x ) .

## 18.2.3 Conditional distribution q ( z t -1  z t )

We dened the conditional probability q ( z t  z t -1 ) as the mixing process (equation 18.2). To reverse this process, we apply Bayes' rule:

<!-- formula-not-decoded -->

This is intractable since we cannot compute the marginal distribution q ( z t -1 ) .

For this simple 1D example, it's possible to evaluate q ( z t -1  z t ) numerically (gure 18.5). In general, their form is complex, but in many cases, they are well-approximated by a normal distribution. This is important because when we build the decoder, we will approximate the reverse process using a normal distribution.

## 18.2.4 Conditional diusion distribution q ( z t -1  z t , x )

There is one nal distribution related to the encoder to consider. We noted above that we could not nd the conditional distribution q ( z t -1  z t ) because we do not know the marginal distribution q ( z t -1 ) . However, if we know the starting variable x , then we do know the distribution q ( z t -1  x ) at the time before. This is just the diusion kernel (gure 18.3), and it is normally distributed.

Hence, it is possible to compute the conditional diusion distribution q ( z t -1  z t , x ) in closed form (gure 18.6). This distribution is used to train the decoder. It is the distribution over z t -1 when we know the current latent variable z t and the training

Appendix C.1.2 Marginalization

Notebook 18.1 Diusion encoder

Appendix C.1.4 Bayes' rule

Figure 18.5 Conditional distribution q z ( t -1  z t ) . a) The marginal densities q z ( t ) with three points z ∗ t highlighted. b) The probability q z ( t -1  z ∗ t ) (cyan curves) is computed via Bayes' rule and is proportional to q z ( ∗ t  z t -1 ) q z ( t -1 ) . In general, it is not normally distributed (top graph), although often the normal is a good approximation (bottom two graphs). The rst likelihood term q z ( ∗ t  z t -1 ) is normal in z t -1 (equation 18.2) with a mean that is slightly further from zero than z ∗ t (brown curves). The second term is the marginal density q z ( t -1 ) (gray curves).

<!-- image -->

Figure 18.6 Conditional distribution q z ( t -1  z , x t ) . a) Diusion kernel for x ∗ = -2 1  with three points z ∗ t highlighted. b) The probability q z ( t - ∗ 1  z ∗ t , x ∗ ) is computed via Bayes' rule and is proportional to q z ( ∗ t  z t -1 ) q z ( t -1  x ) . This is normally distributed and can be computed in closed form. The rst likelihood term q z ( ∗ t  z t -1 ) is normal in z t -1 (equation 18.2) with a mean that is slightly further from zero than z ∗ t (brown curves). The second term is the diusion kernel q z ( t -1  x ∗ ) (gray curves).

<!-- image -->

data example x (which, of course, we do when training). To compute an expression for q ( z t -1  z t , x ) we start with Bayes' rule:

<!-- formula-not-decoded -->

where between the rst two lines, we have used the fact that q ( z t  z t -1 , x ) = q ( z t  z t -1 ) because the diusion process is Markov, and all information about z t is captured by z t -1 . Between lines three and four, we use the Gaussian change of variables identity:

<!-- formula-not-decoded -->

to rewrite the rst distribution in terms of z t -1 . We then use a second Gaussian identity:

<!-- formula-not-decoded -->

to combine the two normal distributions in z t -1 , which gives:

<!-- formula-not-decoded -->

Note that the constants of proportionality in equations 18.12, 18.13, and 18.14 must cancel out since the nal result is already a correctly normalized probability distribution.

## 18.3 Decoder model (reverse process)

When we learn a diusion model, we learn the reverse process . In other words, we learn a series of probabilistic mappings back from latent variable z T to z T -1 , from z T -1 to z T -2 , and so on, until we reach the data x . The true reverse distributions q ( z t -1  z t ) of the diusion process are complex multi-modal distributions (gure 18.5) that depend on the data distribution Pr ( x ) . We approximate these as normal distributions:

<!-- formula-not-decoded -->

Appendix C.3.4 Gaussian change of variables

Problems 18.4-18.5

Problem 18.6

## Appendix C.1.2 Marginalization

where f t [ z t , ϕ t ] is a neural network that computes the mean of the normal distribution in the estimated mapping from z t to the preceding latent variable z t -1 . The terms  σ 2 t  are predetermined. If the hyperparameters β t in the diusion process are close to zero (and the number of time steps T is large), then this normal approximation will be reasonable.

We generate new examples from Pr ( x ) using ancestral sampling. We start by drawing z T from Pr ( z T ) . Then we sample z T -1 from Pr ( z T -1  z T , ϕ T ) , sample z T -2 from Pr ( z T -2  z T -1 , ϕ T -1 ) and so on until we nally generate x from Pr ( x z  1 , ϕ 1 ) .

## 18.4 Training

The joint distribution of the observed variable x and the latent variables  z t  is:

<!-- formula-not-decoded -->

The likelihood of the observed data Pr ( x  ϕ 1  T ) is found by marginalizing over the latent variables:

<!-- formula-not-decoded -->

To train the model, we maximize the log-likelihood of the training data  x i  with respect to the parameters ϕ :

<!-- formula-not-decoded -->

We can't maximize this directly because the marginalization in equation 18.18 is intractable. Hence, we use Jensen's inequality to dene a lower bound on the likelihood and optimize the parameters ϕ 1  T with respect to this bound exactly as we did for the VAE (see section 17.3.1).

## 18.4.1 Evidence lower bound (ELBO)

To derive the lower bound, we multiply and divide the log-likelihood by the encoder distribution q ( z 1  T  x ) and apply Jensen's inequality (see section 17.3.2):

<!-- formula-not-decoded -->

This gives us the evidence lower bound (ELBO):

<!-- formula-not-decoded -->

In the VAE, the encoder q ( z x  ) approximates the posterior distribution over the latent variables to make the bound tight, and the decoder maximizes this bound (gure 17.10). In diusion models, the decoder must do all the work since the encoder has no parameters. It makes the bound tighter by both (i) changing its parameters so that the static encoder does approximate the posterior Pr ( z 1  T  x , ϕ 1  T ) and (ii) optimizing its own parameters with respect to that bound (see gure 17.6).

## 18.4.2 Simplifying the ELBO

We now manipulate the log term from the ELBO into the nal form that we will optimize. We rst substitute in the denitions for the numerator and denominator from equations 18.17 and 18.3, respectively:

<!-- formula-not-decoded -->

Then we expand the denominator of the second term:

<!-- formula-not-decoded -->

Substituting in this result gives:

where the rst equality follows because all of the information about variable z t is encompassed in z t -1 , so the extra conditioning on the data x is irrelevant. The second equality is a straightforward application of Bayes' rule.

<!-- formula-not-decoded -->

Appendix C.1.4 Bayes' rule

Appendix C.5.4 KL divergence between normal distributions

Problem 18.8

where all but two of the terms in the product of the ratios q ( z t -1  x )  q ( z t  x ) cancel out between lines two and three leaving only q ( z 1  x ) and q ( z T  x ) . The last term in the third line is approximately log[1] = 0 since the result of the forward process q ( z T  x ) is a standard normal distribution, and so is equal to the prior Pr ( z T ) .

The simplied ELBO is hence:

Problem 18.7 Appendix C.5.1

where we have marginalized over the irrelevant variables in q ( z 1  T  x ) between lines two and three and used the denition of KL divergence (see problem 18.7).

<!-- formula-not-decoded -->

KL divergence

## 18.4.3 Analyzing the ELBO

The rst probability term in the ELBO was dened in equation 18.16:

<!-- formula-not-decoded -->

The KL divergence terms in the ELBO measure the distance between Pr ( z t -1  z t , ϕ t ) and q ( z t -1  z t , x ) , which were dened in equations 18.16 and 18.15, respectively:

and is equivalent to the reconstruction term in the VAE. The ELBO will be larger if the model prediction matches the observed data. As for the VAE, we will approximate the expectation over the log of this quantity using a Monte Carlo estimate (see equations 17.22-17.23), in which we estimate the expectation with a sample from q ( z 1  x ) .

<!-- formula-not-decoded -->

The KL divergence between two normal distributions has a closed-form expression. Moreover, many of the terms in this expression do not depend on ϕ (see problem 18.8), and the expression simplies to the squared dierence between the means plus a constant C :

<!-- formula-not-decoded -->

Figure 18.7 Fitted Model. a) Individual samples can be generated by sampling from the standard normal distribution Pr z ( T ) (bottom row) and then sampling z T -1 from Pr z ( T -1  z T ) = Norm z T -1 [ f T [ z T , ϕ T ] , σ 2 T I ] and so on until we reach x (ve paths shown). The estimated marginal densities (heatmap) are the aggregation of these samples and are similar to the true marginal densities (gure 18.4). b) The estimated distribution Pr z ( t -1  z t ) (brown curve) is a reasonable approximation to the true posterior of the diusion model q z ( t -1  z t ) (cyan curve) from gure 18.5. The marginal distributions Pr z ( t ) and q z ( t ) of the estimated and true models (dark blue and gray curves, respectively) are also similar.

<!-- image -->

## 18.4.4 Diusion loss function

To t the model, we maximize the ELBO with respect to the parameters ϕ 1  T . We recast this as a minimization by multiplying with minus one and approximating the expectations with samples to give the loss function:

## reconstruction term

<!-- formula-not-decoded -->

where x i is the i th data point, and z it is the associated latent variable at diusion step t .

Figure 18.8 Fitted model results. Cyan and brown curves are original and estimated densities and correspond to the top rows of gures 18.4 and 18.7, respectively. Vertical bars are binned samples from the model, generated by sampling from Pr ( z T ) and propagating back through the variables z T -1 , z T -2 ,    as shown for the ve paths in gure 18.7.

<!-- image -->

## 18.4.5 Training procedure

This loss function can be used to train a network for each diusion time step. It minimizes the dierence between the estimate f t [ z t , ϕ t ] of the hidden variable at the previous time step and the most likely value that it took given the ground truth de-noised data x .

Notebook 18.2 1D diusion model

Figures 18.7 and 18.8 show the tted reverse process for the simple 1D example. This model was trained by (i) taking a large dataset of examples x from the original density, (ii) using the diusion kernel to predict many corresponding values for the latent variable z t at each time t , and then (iii) training the models f t [ z t , ϕ t ] to minimize the loss function in equation 18.29. These models were nonparametric (i.e., lookup tables relating 1D input to 1D output), but more typically, they would be deep neural networks.

## 18.5 Reparameterization of loss function

Although the loss function in equation 18.29 can be used, diusion models have been found to work better with a dierent parameterization; the loss function is modied so that the model aims to predict the noise that was mixed with the original data example to create the current variable. Section 18.5.1 discusses reparameterizing the target (rst two terms in second line of equation 18.29), and section 18.5.2 discusses reparameterizing the network (last term in second line of equation 18.29).

## 18.5.1 Reparameterization of target

The original diusion update was given by:

<!-- formula-not-decoded -->

It follows that the data term x in equation 18.28 can be expressed as the diused image minus the noise that was added to it:

<!-- formula-not-decoded -->

Substituting this into the target terms from equation 18.29 gives:

<!-- formula-not-decoded -->

where we have used the fact that √ α  t √ α t -1 = √ 1 -β t between the second and third lines. Simplifying further, we get:

<!-- formula-not-decoded -->

Substituting this back into the loss function (equation 18.29), we have:

where we have multiplied the numerator and denominator of the rst term by √ 1 -β t between lines two and three, multiplied out the terms, and simplied the numerator in the rst term between lines three and four.

<!-- formula-not-decoded -->

## 18.5.2 Reparameterization of network

Now we replace the model ˆ z t -1 = f t [ z t , ϕ t ] with a new model ˆ = ϵ g t [ z t , ϕ t ] , which predicts the noise ϵ that was mixed with x to create z t :

<!-- formula-not-decoded -->

Draft: please send errata to udlbookmail@gmail.com.

Problem 18.9

Problem 18.10

## Problem 18.11

Substituting the new model into equation 18.34 produces the criterion:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Substituting in the denitions of x and f 1 [ z 1 , ϕ 1 ] from equations 18.31 and 18.35, respectively, the rst term simplies to:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where we have disregarded the additive constants C i .

In practice, the scaling factors (which might be dierent at each time step) are ignored, giving an even simpler formulation:

<!-- formula-not-decoded -->

## 18.6 Implementation

This leads to straightforward algorithms for both training the model (algorithm 18.1) and sampling (algorithm 18.2). The training algorithm has the advantages that it is (i) simple to implement and (ii) naturally augments the dataset; we can reuse every original data point x i as many times as we want at each time step with dierent noise instantiations ϵ . The sampling algorithm has the disadvantage that it requires serial processing of many neural networks g t [ z t , ϕ t ] and is hence time-consuming.

## Algorithm 18.1: Diusion model training

```
Input: Training data x Output: Model parameters ϕ t repeat for i  B do // For every training example index in batch t ∼ Uniform [1 ,    T ] // Sample random timestep ϵ ∼ Norm [ 0 I , ] // Sample noise ℓ i = ∥ ∥ ∥ g t [ √ α t x i + √ 1 -α t ϵ ϕ , t ] -ϵ ∥ ∥ ∥ 2 // Compute individual loss Accumulate losses for batch and take gradient step until converged
```

## Algorithm 18.2: Sampling

```
Input: Model, g t [ · , ϕ t ] Output: Sample, x z T ∼ Norm z [ 0 I , ] // Sample last latent variable for t = T    2 do ˆ z t -1 = 1 √ 1 -β t z t -β t √ 1 -α t √ 1 -β t g t [ z t , ϕ t ] // Predict previous latent variable ϵ ∼ Norm ϵ [ 0 I , ] // Draw new noise vector z t -1 = ˆ z t -1 + σ t ϵ // Add noise to previous latent variable x = 1 √ 1 -β 1 z 1 -β 1 √ 1 -α 1 √ 1 -β 1 g 1 [ z 1 , ϕ 1 ] // Generate sample from z 1 without noise
```

## 18.6.1 Application to images

Diusion models have been very successful in modeling image data. Here, we need to construct models that can take a noisy image and predict the noise that was added at each step. The obvious architectural choice for this image-to-image mapping is the UNet (gure 11.10). However, there may be a very large number of diusion steps, and training and storing multiple U-Nets is inecient. The solution is to train a single U-Net that also takes a predetermined vector representing the time step as input (gure 18.9). In practice, this is resized to match the number of channels at each stage of the U-Net and used to oset and/or scale the representation at each spatial position.

A large number of time steps are needed as the conditional probabilities q ( z t -1  z t ) become closer to normal when the hyperparameters β t are close to zero, matching the form of the decoder distributions Pr ( z t -1  z t , ϕ t ) . However, this makes sampling slow. We might have to run the U-Net model through T =1000 steps to generate good images.

## 18.6.2 Improving generation speed

The loss function (equation 18.40) requires the diusion kernel to have the form q ( z t  x ) = Norm [ √ α t x , √ 1 -α t · I ] . The same loss function will be valid for any forward process

Figure 18.9 U-Net as used in diusion models for images. The network aims to predict the noise that was added to the image. It consists of an encoder which reduces the scale and increases the number of channels and a decoder which increases the scale and reduces the number of channels. The encoder representations are concatenated to their partner in the decoder. Connections between adjacent representations consist of residual blocks, and periodic global self-attention in which every spatial position interacts with every other spatial position. A single network is used for all time steps, by passing a sinusoidal time embedding (gure 12.5) through a shallow neural network and adding the result to the channels at every spatial position at every stage of the U-Net.

<!-- image -->

with this relation, and there is a family of such compatible processes. These are all optimized by the same loss function but have dierent rules for the forward process and dierent corresponding rules for how to use the estimated noise g [ z t , ϕ t ] to predict z t -1 from z t in the reverse process (gure 18.10).

Notebook 18.4 Families of diusion models

Among this family are denoising diusion implicit models , which are no longer stochastic after the rst step from x to z 1 , and accelerated sampling models, where the forward process is dened only on a sub-sequence of time steps. This allows a reverse process that skips time steps and hence makes sampling much more ecient; good samples can be created with 50 time steps when the forward process is no longer stochastic. This is much faster than before but still slower than most other generative models.

## 18.6.3 Conditional generation

If the data has associated labels c , these can be exploited to control the generation. Sometimes this can improve generation results in GANs, and we might expect this to be the case in diusion models as well; it's easier to denoise an image if you have some information about what that image contains. One approach to conditional synthesis in diusion models is classier guidance . This modies the denoising update from z t to z t -1 to take into account class information c . In practice, this means adding an extra

Figure 18.10 Dierent diusion processes that are compatible with the same model. a) Five sampled trajectories of the reparameterized model superimposed on the ground truth marginal distributions. Top row represents Pr ( x ) and subsequent rows represent q ( x t ) . b) Histogram of samples generated from reparameterized model plotted alongside ground truth density curve Pr ( x ) . The same trained model is compatible with a family of diusion models (and corresponding updates in the opposite direction), including the denoising diusion implicit model (DDIM), which is deterministic and does not add noise at each step. c) Five trajectories from DDIM. d) Histogram of samples from DDIM. The same model is also compatible with accelerated diusion models that skip inference steps for increased sampling speed. e) Five trajectories from accelerated model. f) Histogram of samples from accelerated model.

<!-- image -->

term into the nal update step in algorithm 18.2 to yield:

<!-- formula-not-decoded -->

The new term depends on the gradient of a classier Pr c (  z t ) that is based on the latent variable z t . This maps features from the downsampling half of the U-Net to the class c . Like the U-Net, it is usually shared across all time steps and takes time as an input. The update from z t to z t -1 now makes the class c more likely.

Classier-free guidance avoids learning a separate classier Pr c (  z t ) but instead incorporates class information into the main model g t [ z t , ϕ t , c ] . In practice, this usually takes the form of adding an embedding based on c to the layers of the U-Net in a similar way to how the time step is added (see gure 18.9). This model is jointly trained on conditional and unconditional objectives by randomly dropping the class information

Figure 18.11 Cascaded conditional generation based on a text prompt. a) A diusion model consisting of a series of U-Nets is used to generate a 64 × 64 image. b) This generation is conditioned on a sentence embedding computed by a language model. c) A higher resolution 256 × 256 image is generated and conditioned on the smaller image and the text encoding. d) This is repeated to create a 1024 × 1024 image. e) Final image sequence. Adapted from Saharia et al. (2022b).

<!-- image -->

Problem 18.12

during training. Hence, it can both generate unconditional or conditional data examples at test time or any weighted combination of the two. This brings a surprising advantage; if the conditioning information is over-weighted, the model tends to produce very high quality but slightly stereotypical examples. This is somewhat analogous to the use of truncation in GANs (gure 15.10).

## 18.6.4 Improving generation quality

As for other generative models, the highest quality results result from applying a combination of tricks and extensions to the basic model. First, it's been noted that it also helps to estimate the variances σ 2 t of the reverse process as well as the mean (i.e., the widths

of the brown normal distributions in gure 18.7). This particularly improves the results when sampling with fewer steps. Second, it's possible to modify the noise schedule in the forward process so that β t varies at each step, and this can also improve results.

Third, to generate high-resolution images, a cascade of diusion models is used. The rst creates a low-resolution image (possibly guided by class information). The subsequent diusion models generate progressively higher-resolution images. They condition on the lower-resolution image by resizing this and appending it to the layers of the constituent U-Net, as well as any other class information (gure 18.11).

Combining all of these techniques allows the generation of very high-quality images. Figure 18.12 shows examples of images generated from a model conditioned on the ImageNet class. It is particularly impressive that the same model can learn to generate such diverse classes. Figure 18.13 shows images generated from a model that is trained to condition on text captions encoded by a language model like BERT, which are inserted into the model in the same way as the time step (gures 18.9 and 18.11). This results in very realistic images that agree with the caption. Since the diusion model is stochastic by nature, it's possible to generate multiple images that are conditioned on the same caption.

## 18.7 Summary

Diusion models map the data examples through a series of latent variables by repeatedly blending the current representation with random noise. After sucient steps, the representation becomes indistinguishable from white noise. Since these steps are small, the reverse denoising process at each step can be approximated with a normal distribution and predicted by a deep learning model. The loss function is based on the evidence lower bound (ELBO) and ultimately results in a simple least-squares formulation.

For image generation, each denoising step is implemented using a U-Net, so sampling is slow compared to other generative models. To improve generation speed, it's possible to change the diusion model to a deterministic formulation, and here sampling with fewer steps works well. Several methods have been proposed to condition generation on class information, images, and text information. Combining these methods produces impressive text-to-image synthesis results.

## Notes

Denoising diusion models were introduced by Sohl-Dickstein et al. (2015), and early related work based on score-matching was carried out by Song &amp; Ermon (2019). Ho et al. (2020) produced image samples that were competitive with GANs and kick-started a wave of interest in this area. Most of the exposition in this chapter, including the original formulation and the reparameterization, is derived from this paper. Dhariwal &amp; Nichol (2021) improved the quality of these results and showed for the rst time that images from diusion models were quantitatively superior to GAN models in terms of Fréchet Inception Distance. At the time

<!-- image -->

Figure 18.12 Conditional generation using classier guidance. Image samples conditioned on dierent ImageNet classes. The same model produces high quality samples of highly varied image classes. Adapted from Dhariwal &amp; Nichol (2021).

transparent sculpture of a duck made out of glass

<!-- image -->

<!-- image -->

A brain rocketship heading towards the moon riding

<!-- image -->

<!-- image -->

<!-- image -->

An angry duck doing weightlifting at the gym heavy

<!-- image -->

<!-- image -->

<!-- image -->

couple of glasses

New York skyline with Hello World

<!-- image -->

Figure 18.13 Conditional generation using text prompts. Synthesized images from a cascaded generation framework, conditioned on a text prompt encoded by a large language model. The stochastic model can produce many dierent images compatible with the prompt. The model can count objects and incorporate text into images. Adapted from Saharia et al. (2022b).

of writing, the state-of-the-art results for conditional image synthesis have been achieved by Karras et al. (2022). Surveys of denoising diusion models can be found in Croitoru et al. (2022), Cao et al. (2022), Luo (2022), and Yang et al. (2022).

Applications for images: Applications of diusion models include text-to-image generation (Nichol et al., 2022; Ramesh et al., 2022; Saharia et al., 2022b), image-to-image tasks such as colorization, inpainting, uncropping and restoration (Saharia et al., 2022a), super-resolution (Saharia et al., 2022c), image editing (Hertz et al., 2022; Meng et al., 2021), removing adversarial perturbations (Nie et al., 2022), semantic segmentation (Baranchuk et al., 2022), and medical imaging (Song et al., 2021b; Chung &amp; Ye, 2022; Chung et al., 2022; Peng et al., 2022; Xie &amp; Li, 2022; Luo et al., 2022) where the diusion model is sometimes used as a prior.

Dierent data types: Diusion models have also been applied to video data (Ho et al., 2022b; Harvey et al., 2022; Yang et al., 2022; Höppe et al., 2022; Voleti et al., 2022) for generation, past and future frame prediction, and interpolation. They have been used for 3D shape generation (Zhou et al., 2021; Luo &amp; Hu, 2021), and recently a technique has been introduced to generate 3D models using only a 2D text-to-image diusion model (Poole et al., 2023). Austin et al. (2021) and Hoogeboom et al. (2021) investigated diusion models for discrete data. Kong et al. (2021) and Chen et al. (2021d) applied diusion models to audio data.

Alternatives to denoising: The diusion models in this chapter mix noise with the data and build a model to gradually denoise the result. However, degrading the image using noise is not necessary. Rissanen et al. (2022) devised a method that progressively blurred the image and Bansal et al. (2022) showed that the same ideas work with a large family of degradations that do not have to be stochastic. These include masking, morphing, blurring, and pixelating.

Comparison to other generative models: Diusion models synthesize higher quality images than other generative models and are simple to train. They can be thought of as a special case of a hierarchical VAE (Vahdat &amp; Kautz, 2020; Sønderby et al., 2016b) where the encoder is xed, and the latent space is the same size as the data. They are probabilistic, but in their basic form, they can only compute a lower bound on the likelihood of a data point. However, Kingma et al. (2021) show that this lower bound improves on the exact log-likelihoods for test data from normalizing ows and autoregressive models. The likelihood for diusion models can be computed by converting to an ordinary dierential equation (Song et al., 2021c) or by training a continuous normalizing ow model with a diusion-based criterion (Lipman et al., 2022). The main disadvantages of diusion models are that they are slow and that the latent space has no semantic interpretation.

Improving quality: Many techniques have been proposed to improve image quality. These include the reparameterization of the network described in section 18.5 and the equal weighting of the subsequent terms (Ho et al., 2020). Choi et al. (2022) subsequently investigated dierent weightings of terms in the loss function.

Kingma et al. (2021) improved the test log-likelihood of the model by learning the denoising weights β t . Conversely, Nichol &amp; Dhariwal (2021) improved performance by learning separate variances σ 2 of the denoising estimate at each time step in addition to the mean. Bao et al. (2022) show how to learn the variances after training the model.

Ho et al. (2022a) developed the cascaded method for producing very high-resolution images (gure 18.11). To prevent artifacts in lower-resolution images from being propagated to higher resolutions, they introduced noise conditioning augmentation ; here, the lower-resolution conditioning image is degraded by adding noise at each training step. This reduces the reliance on the exact details of the lower-resolution image during training. It is also done during inference, where the best noise level is chosen by sweeping over dierent values.

Improving speed: One of the major drawbacks of diusion models is that they take a long time to train and sample from. Stable diusion (Rombach et al., 2022) projects the original data to a smaller latent space using a conventional autoencoder and then runs the diusion process in this smaller space. This has the advantages of reducing the dimensionality of the training data for the diusion process and allowing other data types (text, graphs, etc.) to be described by diusion models. Vahdat et al. (2021) applied a similar approach.

Song et al. (2021a) showed that an entire family of diusion processes is compatible with the training objective. Most of these processes are non-Markovian (i.e., the diusion step does not only depend on the results of the previous step). One of these models is the denoising diusion implicit model (DDIM), in which the updates are not stochastic (gure 18.10c). This model is amenable to taking larger steps (gure 18.10e) without inducing large errors. It eectively converts the model into an ordinary dierential equation (ODE) in which the trajectories have low curvature and allows ecient numerical methods for solving ODEs to be applied.

Song et al. (2021c) propose converting the underlying stochastic dierential equations into a probability ow ODE which has the same marginal distributions as the original process. Vahdat et al. (2021), Xiao et al. (2022b), and Karras et al. (2022) all exploit techniques for solving ODEs to speed up synthesis. Karras et al. (2022) identied the best-performing time discretization for sampling and evaluated dierent sampler schedules. The result of these and other improvements has been a signicant drop in steps required during synthesis.

Sampling is slow because many small diusion steps are required to ensure that the posterior distribution q ( z t -1  z t ) is close to Gaussian (gure 18.5), so the Gaussian distribution in the decoder is appropriate. If we use a model that describes a more complex distribution at each denoising step, then we can use fewer diusion steps in the rst place. To this end, Xiao et al. (2022b) have investigated using conditional GAN models, and Gao et al. (2021) investigated using conditional energy-based models. Although these models cannot describe the original data distribution, they suce to predict the (much simpler) reverse diusion step.

Salimans &amp; Ho (2022) distilled adjacent steps of the denoising process into a single step to speed up synthesis. Dockhorn et al. (2022) introduced momentum into the diusion process. This makes the trajectories smoother and so more amenable to coarse sampling.

Conditional generation: Dhariwal &amp; Nichol (2021) introduced classier guidance, in which a classier learns to identify the category of object being synthesized at each step, and this is used to bias the denoising update toward that class. This works well, but training a separate classier is expensive. Classier-free guidance (Ho &amp; Salimans, 2022) concurrently trains conditional and unconditional denoising models by dropping the class information some proportion of the time in a process akin to dropout. This technique allows control of the relative contributions of the conditional and unconditional components. Over-weighting the conditional component causes the model to produce more typical and realistic samples.

The standard technique for conditioning on images is to append the (resized) image to the dierent layers of the U-Net. For example, this was used in the cascaded generation process for super-resolution (Ho et al., 2022a). Choi et al. (2021) provide a method for conditioning on images in an unconditional diusion model by matching the latent variables with those of a conditioning image. The standard technique for conditioning on text is to linearly transform the text embedding to the same size as the U-Net layer and then add it to the representation in the same way that the time embedding is introduced (gure 18.9).

Existing diusion models can also be ne-tuned to be conditioned on edge maps, joint positions, segmentation, depth maps, etc., using a neural network structure called a control network (Zhang &amp; Agrawala, 2023).

Text-to-image: Before diusion models, state-of-the-art text-to-image systems were based on transformers (e.g., Ramesh et al., 2021). GLIDE (Nichol et al., 2022) and Dall E 2 (Ramesh · et al., 2022) are both conditioned on embeddings from the CLIP model (Radford et al., 2021),

which generates joint embeddings for text and image data. Imagen (Saharia et al., 2022b) showed that text embeddings from a large language model could produce even better results (see gure 18.13). The same authors introduced a benchmark (DrawBench) which is designed to evaluate the ability of a model to render colors, numbers of objects, spatial relations, and other characteristics. Feng et al. (2022) developed a Chinese text-to-image model.

Connections to other models: This chapter described diusion models as hierarchical variational autoencoders because this approach connects most closely with the other parts of this book. However, diusion models also have close connections with stochastic dierential equations (consider the paths in gure 18.5) and with score matching (Song &amp; Ermon, 2019, 2020). Song et al. (2021c) presented a framework based on stochastic dierential equations that encompasses both the denoising and score matching interpretations. Diusion models also have close connections to normalizing ows (Zhang &amp; Chen, 2021). Yang et al. (2022) present an overview of the relationship between diusion models and other generative approaches.

## Problems

Problem 18.1 Show that if Cov [ x t -1 ] = I and we use the update:

<!-- formula-not-decoded -->

then Cov [ x t ] = I , so the variance stays the same.

Problem 18.2 Consider the variable:

<!-- formula-not-decoded -->

where both ϵ 1 and ϵ 2 are drawn from independent standard normal distributions with mean zero and unit variance. Show that:

<!-- formula-not-decoded -->

so we could equivalently compute z = √ a 2 + b 2 · ϵ where ϵ is also drawn from a standard normal distribution.

Problem 18.3 Continue the process in equation 18.5 to show that:

<!-- formula-not-decoded -->

where ϵ ′ is a draw from a standard normal distribution.

Problem 18.4 ∗ Prove the relation:

<!-- formula-not-decoded -->

Problem 18.5 ∗ Prove the relation:

<!-- formula-not-decoded -->

Problem 18.6 ∗ Derive equation 18.15.

Problem 18.7 ∗ Derive the third line of equation 18.25 from the second line.

Problem 18.8 ∗ The KL-divergence between two normal distributions in D dimensions with means a and b and covariance matrices A and B is given by:

<!-- formula-not-decoded -->

Substitute the denitions from equation 18.27 into this expression and show that the only term that depends on the parameters ϕ is the rst term from equation 18.28.

Problem 18.9 ∗ If α t =  t s =1 1 -β s , then show that:

Problem 18.10 ∗ If α t =  t s =1 1 -β s , then show that:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Problem 18.11 ∗ Prove equation 18.38.

Problem 18.12 Classier-free guidance allows us to create more stereotyped 'canonical' images of a given class. When we described transformer decoders, generative adversarial networks, and the GLOW algorithm, we also discussed methods to reduce the amount of variation and produce more stereotyped outputs. What were these? Do you think it's inevitable that we should limit the output of generative models in this way?