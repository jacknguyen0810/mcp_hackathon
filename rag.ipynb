{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agents.experimental import DocAgent\n",
    "import os\n",
    "from autogen.agents.experimental import DocAgent\n",
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {'cache_seed': 42,\n",
    "                    'temperature': 0.9,\n",
    "                    'top_p': 0.05,\n",
    "                    'config_list': [{'model': 'gpt-4o-mini',\n",
    "                                    'api_key': os.getenv('OPENAI_API_KEY'),\n",
    "                                    'api_type': 'openai'}],\n",
    "                    'timeout': 1200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATH = \"data/\"\n",
    "NUMBER = 5\n",
    "QUERY = f\"Create a list of the top {NUMBER} most important concepts. For each concept, explain it in a detailed manner and why it is important.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autogen.agents.experimental.document_agent.chroma_query_engine:Using existing collection summarise from the database.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to DocAgent):\n",
      "\n",
      "ingest all documents in data/ and do Create a list of the top 5 most important concepts. For each concept, explain it in a single sentance and why it is important..\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33m_User\u001b[0m (to chat_manager):\n",
      "\n",
      "ingest all documents in data/ and do Create a list of the top 5 most important concepts. For each concept, explain it in a single sentance and why it is important..\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: DocumentTriageAgent\n",
      "\u001b[0m\n",
      "\u001b[33mDocumentTriageAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "{\"ingestions\":[{\"path_or_url\":\"data/\"}],\"queries\":[{\"query_type\":\"RAG_QUERY\",\"query\":\"Create a list of the top 5 most important concepts and explain each in a single sentence with its importance.\"}]}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: TaskManagerAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mTaskManagerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_Bt3421rqSlGnF6s140zcbYfH): initiate_tasks *****\u001b[0m\n",
      "Arguments: \n",
      "{\"task_init_info\":{\"ingestions\":[{\"path_or_url\":\"data/\"}],\"queries\":[{\"query_type\":\"RAG_QUERY\",\"query\":\"Create a list of the top 5 most important concepts and explain each in a single sentence with its importance.\"}]}}\n",
      "\u001b[32m*******************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: _Group_Tool_Executor\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION initiate_tasks...\n",
      "Call ID: call_Bt3421rqSlGnF6s140zcbYfH\n",
      "Input arguments: {'task_init_info': {'ingestions': [{'path_or_url': 'data/'}], 'queries': [{'query_type': 'RAG_QUERY', 'query': 'Create a list of the top 5 most important concepts and explain each in a single sentence with its importance.'}]}}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_Bt3421rqSlGnF6s140zcbYfH) *****\u001b[0m\n",
      "Updated context variables with task decisions\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: TaskManagerAgent\n",
      "\u001b[0m\n",
      "\u001b[33mTaskManagerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "[Handing off to DoclingDocIngestAgent]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: DoclingDocIngestAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autogen.agents.experimental.document_agent.document_utils:Detected directory. Listing files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDoclingDocIngestAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_h3LyBK3AAz6jg7IdzYrcJINi): data_ingest_task *****\u001b[0m\n",
      "Arguments: \n",
      "{}\n",
      "\u001b[32m*********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: _Group_Tool_Executor\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION data_ingest_task...\n",
      "Call ID: call_h3LyBK3AAz6jg7IdzYrcJINi\n",
      "Input arguments: {}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash abf069e0fb0f219e247d3d2b243ad857\n",
      "/home/ljf1/dis/MCPAgents/mcp_env/lib/python3.12/site-packages/docling/models/easyocr_model.py:69: UserWarning: Deprecated field. Better to set the `accelerator_options.device` in `pipeline_options`. When `use_gpu and accelerator_options.device == AcceleratorDevice.CUDA` the GPU is used to run EasyOCR. Otherwise, EasyOCR runs in CPU.\n",
      "  warnings.warn(\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document Diffusion model - Wikipedia.pdf\n",
      "INFO:docling.document_converter:Finished converting document Diffusion model - Wikipedia.pdf in 39.36 sec.\n",
      "INFO:docling.pipeline.base_pipeline:Processing document UnderstandingDeepLearning_DiffusionModels.pdf\n",
      "INFO:docling.document_converter:Finished converting document UnderstandingDeepLearning_DiffusionModels.pdf in 39.77 sec.\n",
      "INFO:autogen.agents.experimental.document_agent.parser_utils:Document converted in 79.27 seconds.\n",
      "DEBUG:autogen.agents.experimental.document_agent.parser_utils:Document Diffusion model - Wikipedia.pdf converted.\n",
      "Saved markdown output to: /home/ljf1/dis/mcp_hackathon/parsed_docs\n",
      "DEBUG:autogen.agents.experimental.document_agent.parser_utils:item-0 at level 0: unspecified: group _root_\n",
      "  item-1 at level 1: picture\n",
      "  item-2 at level 1: section_header: Diffusion model\n",
      "  item-3 at level 1: text: In ma ... nd quality.\n",
      "  item-4 at level 1: text: There ... ansformers.\n",
      "  item-5 at level 1: text: As of ...  the image.\n",
      "  item-6 at level 1: text: Diffu ... ration. [6]\n",
      "  item-7 at level 1: text: Other ... g. [13][14]\n",
      "  item-8 at level 1: section_header: Denoi ... usion model\n",
      "  item-9 at level 1: section_header: Non-e ... rmodynamics\n",
      "  item-10 at level 1: text: Diffu ... usion. [15]\n",
      "  item-11 at level 1: text: Consi ... on. This is\n",
      "  item-12 at level 1: text: studi ... stribution.\n",
      "  item-13 at level 1: text: The e ... stribution.\n",
      "  item-14 at level 1: section_header: Denoi ... odel (DDPM)\n",
      "  item-15 at level 1: text: The 2 ... ce. [4][16]\n",
      "  item-16 at level 1: section_header: Forward diffusion\n",
      "  item-17 at level 1: text: To pr ... e notation.\n",
      "  item-18 at level 1: list: group list\n",
      "    item-19 at level 2: list_item: ▪ are ...  constants.\n",
      "    item-20 at level 2: list_item: ▪\n",
      "    item-21 at level 2: list_item: ▪\n",
      "    item-22 at level 2: list_item: ▪\n",
      "    item-23 at level 2: list_item: ▪\n",
      "  item-24 at level 1: formula: \n",
      "  item-25 at level 1: list: group list\n",
      "    item-26 at level 2: list_item: ▪ is  ... ensity at .\n",
      "    item-27 at level 2: list_item: ▪ A v ... nditioning.\n",
      "  item-28 at level 1: text: A for ... erging to .\n",
      "  item-29 at level 1: text: The e ... atisfies or\n",
      "  item-30 at level 1: text: where ... an process,\n",
      "  item-31 at level 1: text: In pa ... ginal gone.\n",
      "  item-32 at level 1: text: For e ... e   steps .\n",
      "  item-33 at level 1: section_header: Deriv ... eterization\n",
      "  item-34 at level 1: text: We kn ... terization:\n",
      "  item-35 at level 1: text: where ...  gaussians.\n",
      "  item-36 at level 1: text: There ...  symmetric.\n",
      "  item-37 at level 1: text: By pl ... terization:\n",
      "  item-38 at level 1: text: where ... riance one.\n",
      "  item-39 at level 1: text: To fi ... nal matrix:\n",
      "  item-40 at level 1: text: Since ... rix must be\n",
      "  item-41 at level 1: text: and s ...  transpose,\n",
      "  item-42 at level 1: text: Plugg ... ng, we have\n",
      "  item-43 at level 1: section_header: Backward diffusion\n",
      "  item-44 at level 1: text: The k ...  defined by\n",
      "  item-45 at level 1: text: The g ...  inference.\n",
      "  item-46 at level 1: section_header: Varia ... l inference\n",
      "  item-47 at level 1: text: The E ... ion, we get\n",
      "  item-48 at level 1: text: We se ...  inference.\n",
      "  item-49 at level 1: text: Defin ... ied to [17]\n",
      "  item-50 at level 1: picture\n",
      "  item-51 at level 1: text: where ... nored. This\n",
      "  item-52 at level 1: formula: \n",
      "  item-53 at level 1: section_header: Noise ... ion network\n",
      "  item-54 at level 1: text: Since ... stimating .\n",
      "  item-55 at level 1: text: There ...  it predict\n",
      "  item-56 at level 1: text: It re ... erformance.\n",
      "  item-57 at level 1: text: With  ... ss function\n",
      "  item-58 at level 1: text: resul ... ter models.\n",
      "  item-59 at level 1: section_header: Backw ... ion process\n",
      "  item-60 at level 1: text: After ... as follows:\n",
      "  item-61 at level 1: list: group list\n",
      "    item-62 at level 2: list_item: 1. Co ... se estimate\n",
      "    item-63 at level 2: list_item: 2. Co ... ta estimate\n",
      "    item-64 at level 2: list_item: 3. Sa ... evious data\n",
      "    item-65 at level 2: list_item: 4. Change time\n",
      "  item-66 at level 1: section_header: Score ... ative model\n",
      "  item-67 at level 1: text: Score ... 19][20][21]\n",
      "  item-68 at level 1: section_header: Score matching\n",
      "  item-69 at level 1: section_header: The i ... e functions\n",
      "  item-70 at level 1: text: Consi ... y a certain\n",
      "  item-71 at level 1: text: image ... in general.\n",
      "  item-72 at level 1: text: Most  ... oise added?\n",
      "  item-73 at level 1: text: Conse ... or effects:\n",
      "  item-74 at level 1: list: group list\n",
      "    item-75 at level 2: list_item: ▪ One ... any , where\n",
      "    item-76 at level 2: list_item: is an ... cern to us.\n",
      "    item-77 at level 2: list_item: ▪ Two ... ghbors , by\n",
      "  item-78 at level 1: text: Let t ... n do with .\n",
      "  item-79 at level 1: text: As it ... s exactly .\n",
      "  item-80 at level 1: text: There ... ion to as .\n",
      "  item-81 at level 1: section_header: Learn ... re function\n",
      "  item-82 at level 1: text: Given ... nt descent.\n",
      "  item-83 at level 1: section_header: Annea ... re function\n",
      "  item-84 at level 1: text: Suppo ... a particle:\n",
      "  item-85 at level 1: text: To de ...  diffusion.\n",
      "  item-86 at level 1: section_header: Conti ... n processes\n",
      "  item-87 at level 1: section_header: Forwa ... ion process\n",
      "  item-88 at level 1: text: Consi ... nuous time:\n",
      "  item-89 at level 1: text: By ta ... l equation:\n",
      "  item-90 at level 1: text: where ... an motion).\n",
      "  item-91 at level 1: text: Now,  ... ion models.\n",
      "  item-92 at level 1: text: Now t ...  beginning.\n",
      "  item-93 at level 1: text: By Fo ... uivalently,\n",
      "  item-94 at level 1: section_header: Backw ... ion process\n",
      "  item-95 at level 1: text: If we ...  Suppose we\n",
      "  item-96 at level 1: text: start ... wards. [23]\n",
      "  item-97 at level 1: section_header: Noise ... work (NCSN)\n",
      "  item-98 at level 1: text: At th ... mit, and so\n",
      "  item-99 at level 1: text: In pa ... e for any .\n",
      "  item-100 at level 1: text: Now,  ... divergence:\n",
      "  item-101 at level 1: text: After ... E from to :\n",
      "  item-102 at level 1: text: This  ... ama method.\n",
      "  item-103 at level 1: text: The n ... ained thus:\n",
      "  item-104 at level 1: list: group list\n",
      "    item-105 at level 2: list_item: ▪ \"ne ... al network.\n",
      "    item-106 at level 2: list_item: ▪ \"sc ...  function .\n",
      "    item-107 at level 2: list_item: ▪ \"no ... oise added.\n",
      "  item-108 at level 1: section_header: Their equivalence\n",
      "  item-109 at level 1: text: DDPM  ... vice versa.\n",
      "  item-110 at level 1: text: We kn ... la, we have\n",
      "  item-111 at level 1: text: As de ...  variables,\n",
      "  item-112 at level 1: text: and t ... hen we have\n",
      "  item-113 at level 1: text: Thus, ...  denoising.\n",
      "  item-114 at level 1: text: Conve ...  diffusion:\n",
      "  item-115 at level 1: text: Thus, ...  diffusion.\n",
      "  item-116 at level 1: section_header: Main variants\n",
      "  item-117 at level 1: section_header: Noise schedule\n",
      "  item-118 at level 1: text: In DD ... uantities .\n",
      "  item-119 at level 1: text: In or ... ne trains .\n",
      "  item-120 at level 1: text: Simil ... ne trains .\n",
      "  item-121 at level 1: section_header: Denoi ... odel (DDIM)\n",
      "  item-122 at level 1: text: The o ... ntractable.\n",
      "  item-123 at level 1: picture\n",
      "    item-123 at level 2: caption: Illustration for a linear diffusion noise schedule. With settings\n",
      "    item-124 at level 2: caption: Illus ... th settings\n",
      "  item-125 at level 1: caption: .\n",
      "  item-126 at level 1: text: DDIM  ... forms DDPM.\n",
      "  item-127 at level 1: text: In    ... d sample is\n",
      "  item-128 at level 1: text: where ... high-level.\n",
      "  item-129 at level 1: text: The o ... tween them.\n",
      "  item-130 at level 1: text: By th ... ion models.\n",
      "  item-131 at level 1: section_header: Laten ... model (LDM)\n",
      "  item-132 at level 1: text: Since ... image. [26]\n",
      "  item-133 at level 1: text: The e ... oder (VAE).\n",
      "  item-134 at level 1: section_header: Archi ... mprovements\n",
      "  item-135 at level 1: text: [27]  ... parameter .\n",
      "  item-136 at level 1: text: In th ...  true. [29]\n",
      "  item-137 at level 1: section_header: Classifier guidance\n",
      "  item-138 at level 1: text: Class ... ation. [30]\n",
      "  item-139 at level 1: text: Suppo ... scription).\n",
      "  item-140 at level 1: text: Takin ... ad in mind.\n",
      "  item-141 at level 1: text: In ot ... replaced by\n",
      "  item-142 at level 1: text: where ... classifier.\n",
      "  item-143 at level 1: text: Durin ... ime, giving\n",
      "  item-144 at level 1: text: Altho ... hich case .\n",
      "  item-145 at level 1: text: Class ... on network,\n",
      "  item-146 at level 1: text: but   ... ction: [30]\n",
      "  item-147 at level 1: section_header: With temperature\n",
      "  item-148 at level 1: text: The c ... mages. [30]\n",
      "  item-149 at level 1: text: This  ... s equation:\n",
      "  item-150 at level 1: text: For d ... nds to [31]\n",
      "  item-151 at level 1: section_header: Class ... dance (CFG)\n",
      "  item-152 at level 1: text: If we ... tself: [31]\n",
      "  item-153 at level 1: text: Such  ...  both and .\n",
      "  item-154 at level 1: text: Note  ... rediction .\n",
      "  item-155 at level 1: text: For d ... responds to\n",
      "  item-156 at level 1: text: As sa ... ten as [32]\n",
      "  item-157 at level 1: picture\n",
      "  item-158 at level 1: text: A   s ... n. [33][34]\n",
      "  item-159 at level 1: section_header: Samplers\n",
      "  item-160 at level 1: text: Given ... oise level:\n",
      "  item-161 at level 1: text: A noi ... lent, since\n",
      "  item-162 at level 1: formula: \n",
      "  item-163 at level 1: text: In th ... s in DDPM).\n",
      "  item-164 at level 1: text: In th ... ation. [36]\n",
      "  item-165 at level 1: text: A sur ... is in. [37]\n",
      "  item-166 at level 1: section_header: Other examples\n",
      "  item-167 at level 1: text: Notab ... 4][45] etc.\n",
      "  item-168 at level 1: section_header: Flow- ... usion model\n",
      "  item-169 at level 1: text: Abstr ...  function .\n",
      "  item-170 at level 1: text: In de ... lation. [2]\n",
      "  item-171 at level 1: text: In fl ... ll-behaved.\n",
      "  item-172 at level 1: text: Given ... city field:\n",
      "  item-173 at level 1: text: we  e ... rticular, .\n",
      "  item-174 at level 1: text: The p ... stribution:\n",
      "  item-175 at level 1: text: To co ... ility path:\n",
      "  item-176 at level 1: text: The c ... ian path is\n",
      "  item-177 at level 1: text: The p ... rginalizing\n",
      "  item-178 at level 1: section_header: Optim ... nsport flow\n",
      "  item-179 at level 1: text: The i ...  transport.\n",
      "  item-180 at level 1: section_header: Rectified flow\n",
      "  item-181 at level 1: text: The i ...  reach such\n",
      "  item-182 at level 1: text: perfe ... ttle steps.\n",
      "  item-183 at level 1: picture\n",
      "  item-184 at level 1: text: The   ... n, we stop.\n",
      "  item-185 at level 1: text: Gener ... by solving:\n",
      "  item-186 at level 1: picture\n",
      "  item-187 at level 1: text: In re ... retization.\n",
      "  item-188 at level 1: text: Speci ... direction :\n",
      "  item-189 at level 1: picture\n",
      "  item-190 at level 1: caption: Trans ... d flow [47]\n",
      "  item-191 at level 1: picture\n",
      "  item-192 at level 1: text: The d ... nding. [51]\n",
      "  item-193 at level 1: text: A dis ... ncreasing .\n",
      "  item-194 at level 1: text: Recti ... ensures   a\n",
      "  item-195 at level 1: picture\n",
      "    item-195 at level 2: caption: The reflow process [47]\n",
      "    item-196 at level 2: caption: The r ... rocess [47]\n",
      "  item-197 at level 1: text: reduc ... s of . [47]\n",
      "  item-198 at level 1: text: See [ ... animations.\n",
      "  item-199 at level 1: section_header: Choic ... rchitecture\n",
      "  item-200 at level 1: section_header: Diffusion model\n",
      "  item-201 at level 1: text: For g ... mages. [53]\n",
      "  item-202 at level 1: text: For D ... plied. [55]\n",
      "  item-203 at level 1: picture\n",
      "    item-203 at level 2: caption: Architecture of Stable Diffusion\n",
      "    item-204 at level 2: caption: Archi ... e Diffusion\n",
      "  item-205 at level 1: picture\n",
      "    item-205 at level 2: caption: The denoising process used by Stable Diffusion\n",
      "    item-206 at level 2: caption: The d ... e Diffusion\n",
      "  item-207 at level 1: text: DDPM  ...  noisy one.\n",
      "  item-208 at level 1: section_header: Conditioning\n",
      "  item-209 at level 1: text: The b ... ds to first\n",
      "  item-210 at level 1: text: conve ... o a vector.\n",
      "  item-211 at level 1: text: Stabl ... olNet. [57]\n",
      "  item-212 at level 1: text: As a  ... iting. [59]\n",
      "  item-213 at level 1: text: Condi ... ry in. [60]\n",
      "  item-214 at level 1: section_header: Upscaling\n",
      "  item-215 at level 1: text: As ge ... resampling.\n",
      "  item-216 at level 1: text: Diffu ... peats. [53]\n",
      "  item-217 at level 1: text: In mo ... llows: [53]\n",
      "  item-218 at level 1: list: group list\n",
      "    item-219 at level 2: list_item: ▪ Sam ... image, etc.\n",
      "    item-220 at level 2: list_item: ▪ Sam ... f the high-\n",
      "  item-221 at level 1: formula: \n",
      "  item-222 at level 1: list: group list\n",
      "    item-223 at level 2: list_item: ▪ Tra ... e L2 loss .\n",
      "  item-224 at level 1: section_header: Examples\n",
      "  item-225 at level 1: text: This  ... chitecture.\n",
      "  item-226 at level 1: section_header: OpenAI\n",
      "  item-227 at level 1: text: The D ...  of images.\n",
      "  item-228 at level 1: text: The f ... s the text.\n",
      "  item-229 at level 1: text: GLIDE ... d \"unCLIP\".\n",
      "  item-230 at level 1: text: The u ... o an image.\n",
      "  item-231 at level 1: text: Sora  ... odel (DiT).\n",
      "  item-232 at level 1: section_header: Stability AI\n",
      "  item-233 at level 1: text: Stabl ... n. [65][26]\n",
      "  item-234 at level 1: text: Stabl ... ified flow.\n",
      "  item-235 at level 1: text: Stabl ... 3D objects.\n",
      "  item-236 at level 1: section_header: Google\n",
      "  item-237 at level 1: text: Image ... all U-Nets.\n",
      "  item-238 at level 1: text: Muse  ... age tokens.\n",
      "  item-239 at level 1: text: Image ... lable. [72]\n",
      "  item-240 at level 1: text: Veo ( ... rompt. [73]\n",
      "  item-241 at level 1: section_header: Meta\n",
      "  item-242 at level 1: text: Make- ... l. [74][75]\n",
      "  item-243 at level 1: text: CM3le ... 2. [76][77]\n",
      "  item-244 at level 1: text: Trans ... tion). [78]\n",
      "  item-245 at level 1: picture\n",
      "    item-245 at level 2: caption: Transfusion architectural diagram\n",
      "    item-246 at level 2: caption: Trans ... ral diagram\n",
      "  item-247 at level 1: text: Movie ... ching. [79]\n",
      "  item-248 at level 1: section_header: See also\n",
      "  item-249 at level 1: list: group list\n",
      "    item-250 at level 2: list_item: ▪ Diffusion process\n",
      "    item-251 at level 2: list_item: ▪ Markov chain\n",
      "    item-252 at level 2: list_item: ▪ Var ... l inference\n",
      "    item-253 at level 2: list_item: ▪ Var ... autoencoder\n",
      "  item-254 at level 1: section_header: Further reading\n",
      "  item-255 at level 1: list: group list\n",
      "    item-256 at level 2: list_item: ▪ Review papers\n",
      "    item-257 at level 2: list_item: ▪ Yan ...  2024-09-06\n",
      "    item-258 at level 2: list_item: ▪ Yan ... 360-030 0).\n",
      "    item-259 at level 2: list_item: ▪ Aus ... ve/cs.LG)].\n",
      "    item-260 at level 2: list_item: ▪ Cro ... /37030794).\n",
      "    item-261 at level 2: list_item: ▪ Mat ... he article.\n",
      "  item-262 at level 1: list: group list\n",
      "    item-263 at level 2: list_item: ▪ \"Po ... 2023-09-25.\n",
      "    item-264 at level 2: list_item: ▪ Luo ... ve/cs.LG)].\n",
      "    item-265 at level 2: list_item: ▪ Wen ... 2023-09-25.\n",
      "  item-266 at level 1: section_header: ▪ Tutorials\n",
      "  item-267 at level 1: list: group list\n",
      "    item-268 at level 2: list_item: ▪ Nak ... ve/cs.LG)].\n",
      "    item-269 at level 2: list_item: ▪ \"Gu ... al details.\n",
      "  item-270 at level 1: section_header: References\n",
      "  item-271 at level 1: list: group list\n",
      "    item-272 at level 2: list_item: 1. Ch ... ve/cs.LG)].\n",
      "    item-273 at level 2: list_item: 2. So ... ve/cs.LG)].\n",
      "    item-274 at level 2: list_item: 3. Cr ... 252199918).\n",
      "    item-275 at level 2: list_item: 4. Ho ...  6840-6851.\n",
      "    item-276 at level 2: list_item: 5. Gu ... ve/cs.CV)].\n",
      "    item-277 at level 2: list_item: 6. GL ...  2023-09-24\n",
      "    item-278 at level 2: list_item: 7. Ni ... ve/cs.AI)].\n",
      "    item-279 at level 2: list_item: 8. Li ... 56792-03-4.\n",
      "    item-280 at level 2: list_item: 9. Ha ... -long.647).\n",
      "  item-281 at level 1: list: group list\n",
      "    item-282 at level 2: list_item: 10. X ... emnlp.606).\n",
      "    item-283 at level 2: list_item: 11. Z ... s-acl.828).\n",
      "    item-284 at level 2: list_item: 12. Y ... 32 9-9290).\n",
      "    item-285 at level 2: list_item: 13. J ... ve/cs.LG)].\n",
      "    item-286 at level 2: list_item: 14. C ... ve/cs.RO)].\n",
      "    item-287 at level 2: list_item: 15. S ... 503.03585).\n",
      "    item-288 at level 2: list_item: 16. H ...  2024-09-07\n",
      "    item-289 at level 2: list_item: 17. W ... 2023-09-24.\n",
      "    item-290 at level 2: list_item: 18. \" ... 2023-09-24.\n",
      "    item-291 at level 2: list_item: 19. S ... 907.05600).\n",
      "    item-292 at level 2: list_item: 20. S ... ve/cs.LG)].\n",
      "    item-293 at level 2: list_item: 21. e ...  2024-09-07\n",
      "    item-294 at level 2: list_item: 22. \" ... 2023-09-24.\n",
      "    item-295 at level 2: list_item: 23. A ... 0304-4149).\n",
      "    item-296 at level 2: list_item: 24. L ... ve/cs.LG)].\n",
      "    item-297 at level 2: list_item: 25. S ... ve/cs.LG)].\n",
      "  item-298 at level 1: list: group list\n",
      "    item-299 at level 2: list_item: 26. R ... ve/cs.CV)].\n",
      "    item-300 at level 2: list_item: 27. N ...  8162-8171.\n",
      "    item-301 at level 2: list_item: 28. S ... ICLR 2022).\n",
      "    item-302 at level 2: list_item: 29. L ...  5404-5411.\n",
      "    item-303 at level 2: list_item: 30. D ... ve/cs.LG)].\n",
      "    item-304 at level 2: list_item: 31. H ... ve/cs.LG)].\n",
      "    item-305 at level 2: list_item: 32. C ... ve/cs.CV)].\n",
      "    item-306 at level 2: list_item: 33. S ... ve/cs.CL)].\n",
      "    item-307 at level 2: list_item: 34. A ... ve/cs.CV)].\n",
      "    item-308 at level 2: list_item: 35. Y ... e/c s.CV)].\n",
      "    item-309 at level 2: list_item: 36. S ... ve/cs.LG)].\n",
      "    item-310 at level 2: list_item: 37. K ... ve/cs.CV)].\n",
      "    item-311 at level 2: list_item: 38. C ... 1041-4347).\n",
      "    item-312 at level 2: list_item: 39. X ... 302.04265).\n",
      "    item-313 at level 2: list_item: 40. S ... 2211-32252.\n",
      "  item-314 at level 1: list: group list\n",
      "    item-315 at level 2: list_item: 41. D ... /stat.ML)].\n",
      "    item-316 at level 2: list_item: 42. L ... ve/cs.LG)].\n",
      "    item-317 at level 2: list_item: 43. B ... 208.09392).\n",
      "    item-318 at level 2: list_item: 44. G ... 305.18619).\n",
      "    item-319 at level 2: list_item: 45. L ... /stat.ML)].\n",
      "    item-320 at level 2: list_item: 46. T ... 2835-8856).\n",
      "    item-321 at level 2: list_item: 47. L ... ve/cs.LG)].\n",
      "    item-322 at level 2: list_item: 48. L ... /stat.ML)].\n",
      "    item-323 at level 2: list_item: 49. L ... ve/cs.LG)].\n",
      "    item-324 at level 2: list_item: 50. A ... e/c s.LG)].\n",
      "    item-325 at level 2: list_item: 51. H ... ve/cs.GR)].\n",
      "    item-326 at level 2: list_item: 52. \" ... 2024-08-20.\n",
      "    item-327 at level 2: list_item: 53. H ... 1532-4435).\n",
      "    item-328 at level 2: list_item: 54. P ... ve/cs.CV)].\n",
      "    item-329 at level 2: list_item: 55. F ... ve/cs.CV)].\n",
      "  item-330 at level 1: list: group list\n",
      "    item-331 at level 2: list_item: 56. T ... ve/cs.CV)].\n",
      "    item-332 at level 2: list_item: 57. Z ...  e/cs.CV)].\n",
      "    item-333 at level 2: list_item: 58. L ... ve/cs.CV)].\n",
      "    item-334 at level 2: list_item: 59. H ... ve/cs.CV)].\n",
      "    item-335 at level 2: list_item: 60. Z ... /stat.ML)].\n",
      "    item-336 at level 2: list_item: 61. W ... 107.10833).\n",
      "    item-337 at level 2: list_item: 62. L ... 8.10257v1).\n",
      "    item-338 at level 2: list_item: 63. N ... ve/cs.CV)].\n",
      "    item-339 at level 2: list_item: 64. R ... ve/cs.CV)].\n",
      "    item-340 at level 2: list_item: 65. A ... 2022-10-31.\n",
      "    item-341 at level 2: list_item: 66. E ... ve/cs.CV)].\n",
      "    item-342 at level 2: list_item: 67. X ... ve/cs.CV)].\n",
      "    item-343 at level 2: list_item: 68. \" ... 2024-04-04.\n",
      "    item-344 at level 2: list_item: 69. S ... 205.11487).\n",
      "  item-345 at level 1: list: group list\n",
      "    item-346 at level 2: list_item: 70. C ... ve/cs.CV)].\n",
      "    item-347 at level 2: list_item: 71. \" ... 2024-04-04.\n",
      "    item-348 at level 2: list_item: 72. I ... name (help)\n",
      "    item-349 at level 2: list_item: 73. \" ... 2024-05-17.\n",
      "    item-350 at level 2: list_item: 74. \" ... 2024-09-20.\n",
      "    item-351 at level 2: list_item: 75. S ... ve/cs.CV)].\n",
      "    item-352 at level 2: list_item: 76. \" ... 2024-09-20.\n",
      "    item-353 at level 2: list_item: 77. C ... ve/cs.CL)].\n",
      "    item-354 at level 2: list_item: 78. Z ... ve/cs.AI)].\n",
      "    item-355 at level 2: list_item: 79. M ... er 4, 2024.\n",
      "  item-356 at level 1: text: Retri ... 1285837703\"\n",
      "DEBUG:autogen.agents.experimental.document_agent.parser_utils:Document UnderstandingDeepLearning_DiffusionModels.pdf converted.\n",
      "Saved markdown output to: /home/ljf1/dis/mcp_hackathon/parsed_docs\n",
      "DEBUG:autogen.agents.experimental.document_agent.parser_utils:item-0 at level 0: unspecified: group _root_\n",
      "  item-1 at level 1: section_header: Chapter 18\n",
      "  item-2 at level 1: section_header: Diusion models\n",
      "  item-3 at level 1: text: Chapt ... ower bound.\n",
      "  item-4 at level 1: text: This  ... is chapter.\n",
      "  item-5 at level 1: section_header: 18.1 Overview\n",
      "  item-6 at level 1: text: A di ... erministic.\n",
      "  item-7 at level 1: text: In th ... etween each\n",
      "  item-8 at level 1: text: The e ... he decoder.\n",
      "  item-9 at level 1: picture\n",
      "    item-9 at level 2: caption: Figure 18.1 Diusion models. The encoder (forward, or diusion process) maps the input x through a series of latent variables z 1    z T . This process is prespecied and gradually mixes the data with noise until only noise remains. The decoder (reverse process) is learned and passes the data back through the latent variables, removing noise at each stage. After training, new examples are generated by sampling noise vectors z T and passing them through the decoder.\n",
      "    item-10 at level 2: caption: Figur ... he decoder.\n",
      "  item-11 at level 1: text: adjac ... he decoder.\n",
      "  item-12 at level 1: text: In se ... xt prompts.\n",
      "  item-13 at level 1: section_header: 18.2  ... rd process)\n",
      "  item-14 at level 1: text: The d ... cording to:\n",
      "  item-15 at level 1: formula: \n",
      "  item-16 at level 1: text: where ... written as:\n",
      "  item-17 at level 1: footnote: 1 Not ... back again.\n",
      "  item-18 at level 1: section_header: Problem 18.1\n",
      "  item-19 at level 1: picture\n",
      "    item-19 at level 2: caption: Figure 18.2 Forward process. a) We consider one-dimensional data x with T = 100 latent variables z , 1    , z 100 and β = 0 03  at all steps. Three values of x (gray, cyan, and orange) are initialized (top row). These are propagated through z , 1    , z 100 . At each step, the variable is updated by attenuating its value by √ 1 -β and adding noise with mean zero and variance β (equation 18.1). Accordingly, the three examples noisily propagate through the variables with a tendency to move toward zero. b) The conditional probabilities Pr z ( 1  x ) and Pr z ( t  z t -1 ) are normal distributions with a mean that is slightly closer to zero than the current point and a xed variance β t (equation 18.2).\n",
      "    item-20 at level 2: caption: Figur ... tion 18.2).\n",
      "  item-21 at level 1: formula: \n",
      "  item-22 at level 1: text: The j ... input x is:\n",
      "  item-23 at level 1: text: This  ... ribution. 2\n",
      "  item-24 at level 1: formula: \n",
      "  item-25 at level 1: footnote: 2 We  ... us chapter.\n",
      "  item-26 at level 1: picture\n",
      "    item-26 at level 2: caption: Figure 18.3 Diusion kernel. a) The point x ∗ =2 0  is propagated through the latent variables using equation 18.1 (ve paths shown in gray). The diusion kernel q z ( t  x ∗ ) is the probability distribution over variable z t given that we started from x ∗ . It can be computed in closed-form and is a normal distribution whose mean moves toward zero and whose variance increases as t increases. Heatmap shows q z ( t  x ∗ ) for each variable. Cyan lines show ± 2 standard deviations from the mean. b) The diusion kernel q z ( t  x ∗ ) is shown explicitly for t = 20 40 80 , , . In practice, the diusion kernel allows us to sample a latent variable z t corresponding to a given x ∗ without computing the intermediate variables z , 1    , z t -1 . When t becomes very large, the diusion kernel becomes a standard normal.\n",
      "    item-27 at level 2: caption: Figur ... ard normal.\n",
      "  item-28 at level 1: picture\n",
      "    item-28 at level 2: caption: Figure 18.4 Marginal distributions. a) Given an initial density Pr x ( ) (top row), the diusion process gradually blurs the distribution as it passes through the latent variables z t and moves it toward a standard normal distribution. Each subsequent horizontal line of heatmap represents a marginal distribution q z ( t ) . b) The top graph shows the initial distribution Pr x ( ) . The other two graphs show the marginal distributions q z ( 20 ) and q z ( 60 ) , respectively.\n",
      "    item-29 at level 2: caption: Figur ... spectively.\n",
      "  item-30 at level 1: section_header: 18.2. ... ( z t  x )\n",
      "  item-31 at level 1: text: To tr ... gure 18.3).\n",
      "  item-32 at level 1: text: To de ... rd process:\n",
      "  item-33 at level 1: formula: \n",
      "  item-34 at level 1: text: Subst ... nd, we get:\n",
      "  item-35 at level 1: formula: \n",
      "  item-36 at level 1: text: The l ...  18.2), so:\n",
      "  item-37 at level 1: formula: \n",
      "  item-38 at level 1: text: If we ...  show that:\n",
      "  item-39 at level 1: text: where ... stribution.\n",
      "  item-40 at level 1: formula: \n",
      "  item-41 at level 1: formula: \n",
      "  item-42 at level 1: text: where ... istic form:\n",
      "  item-43 at level 1: text: For a ... z t  x ) .\n",
      "  item-44 at level 1: section_header: 18.2. ... s q ( z t )\n",
      "  item-45 at level 1: text: The m ... ch starting\n",
      "  item-46 at level 1: section_header: Problem 18.2\n",
      "  item-47 at level 1: section_header: Problem 18.3\n",
      "  item-48 at level 1: text: point ... xcept z t :\n",
      "  item-49 at level 1: formula: \n",
      "  item-50 at level 1: text: where ... ation 18.3.\n",
      "  item-51 at level 1: text: Howev ... ntly write:\n",
      "  item-52 at level 1: formula: \n",
      "  item-53 at level 1: text: Hence ...  Pr ( x ) .\n",
      "  item-54 at level 1: section_header: 18.2. ...  -1  z t )\n",
      "  item-55 at level 1: text: We de ... ayes' rule:\n",
      "  item-56 at level 1: formula: \n",
      "  item-57 at level 1: text: This  ...  z t -1 ) .\n",
      "  item-58 at level 1: text: For t ... stribution.\n",
      "  item-59 at level 1: section_header: 18.2. ...  z t , x )\n",
      "  item-60 at level 1: text: There ... istributed.\n",
      "  item-61 at level 1: text: Hence ... he training\n",
      "  item-62 at level 1: text: Appen ... inalization\n",
      "  item-63 at level 1: text: Noteb ... ion encoder\n",
      "  item-64 at level 1: text: Appen ... Bayes' rule\n",
      "  item-65 at level 1: picture\n",
      "    item-65 at level 2: caption: Figure 18.5 Conditional distribution q z ( t -1  z t ) . a) The marginal densities q z ( t ) with three points z ∗ t highlighted. b) The probability q z ( t -1  z ∗ t ) (cyan curves) is computed via Bayes' rule and is proportional to q z ( ∗ t  z t -1 ) q z ( t -1 ) . In general, it is not normally distributed (top graph), although often the normal is a good approximation (bottom two graphs). The rst likelihood term q z ( ∗ t  z t -1 ) is normal in z t -1 (equation 18.2) with a mean that is slightly further from zero than z ∗ t (brown curves). The second term is the marginal density q z ( t -1 ) (gray curves).\n",
      "    item-66 at level 2: caption: Figur ... ay curves).\n",
      "  item-67 at level 1: picture\n",
      "    item-67 at level 2: caption: Figure 18.6 Conditional distribution q z ( t -1  z , x t ) . a) Diusion kernel for x ∗ = -2 1  with three points z ∗ t highlighted. b) The probability q z ( t - ∗ 1  z ∗ t , x ∗ ) is computed via Bayes' rule and is proportional to q z ( ∗ t  z t -1 ) q z ( t -1  x ) . This is normally distributed and can be computed in closed form. The rst likelihood term q z ( ∗ t  z t -1 ) is normal in z t -1 (equation 18.2) with a mean that is slightly further from zero than z ∗ t (brown curves). The second term is the diusion kernel q z ( t -1  x ∗ ) (gray curves).\n",
      "    item-68 at level 2: caption: Figur ... ay curves).\n",
      "  item-69 at level 1: text: data  ... ayes' rule:\n",
      "  item-70 at level 1: formula: \n",
      "  item-71 at level 1: text: where ... s identity:\n",
      "  item-72 at level 1: formula: \n",
      "  item-73 at level 1: text: to re ... n identity:\n",
      "  item-74 at level 1: formula: \n",
      "  item-75 at level 1: text: to co ... hich gives:\n",
      "  item-76 at level 1: formula: \n",
      "  item-77 at level 1: text: Note  ... stribution.\n",
      "  item-78 at level 1: section_header: 18.3  ... se process)\n",
      "  item-79 at level 1: text: When  ... tributions:\n",
      "  item-80 at level 1: formula: \n",
      "  item-81 at level 1: text: Appen ... f variables\n",
      "  item-82 at level 1: text: Problems 18.4-18.5\n",
      "  item-83 at level 1: text: Problem 18.6\n",
      "  item-84 at level 1: section_header: Appen ... inalization\n",
      "  item-85 at level 1: text: where ... reasonable.\n",
      "  item-86 at level 1: text: We ge ... 1 , ϕ 1 ) .\n",
      "  item-87 at level 1: section_header: 18.4 Training\n",
      "  item-88 at level 1: text: The j ...  z t  is:\n",
      "  item-89 at level 1: formula: \n",
      "  item-90 at level 1: text: The l ...  variables:\n",
      "  item-91 at level 1: formula: \n",
      "  item-92 at level 1: text: To tr ... ameters ϕ :\n",
      "  item-93 at level 1: formula: \n",
      "  item-94 at level 1: text: We ca ... on 17.3.1).\n",
      "  item-95 at level 1: section_header: 18.4. ... ound (ELBO)\n",
      "  item-96 at level 1: text: To de ... on 17.3.2):\n",
      "  item-97 at level 1: formula: \n",
      "  item-98 at level 1: text: This  ... und (ELBO):\n",
      "  item-99 at level 1: formula: \n",
      "  item-100 at level 1: text: In th ... gure 17.6).\n",
      "  item-101 at level 1: section_header: 18.4. ... ng the ELBO\n",
      "  item-102 at level 1: text: We no ... spectively:\n",
      "  item-103 at level 1: formula: \n",
      "  item-104 at level 1: text: Then  ... econd term:\n",
      "  item-105 at level 1: formula: \n",
      "  item-106 at level 1: text: Subst ... sult gives:\n",
      "  item-107 at level 1: text: where ... ayes' rule.\n",
      "  item-108 at level 1: formula: \n",
      "  item-109 at level 1: text: Appen ... Bayes' rule\n",
      "  item-110 at level 1: text: Appen ... stributions\n",
      "  item-111 at level 1: text: Problem 18.8\n",
      "  item-112 at level 1: text: where ... r ( z T ) .\n",
      "  item-113 at level 1: text: The s ... O is hence:\n",
      "  item-114 at level 1: text: Probl ... endix C.5.1\n",
      "  item-115 at level 1: text: where ... blem 18.7).\n",
      "  item-116 at level 1: formula: \n",
      "  item-117 at level 1: text: KL divergence\n",
      "  item-118 at level 1: section_header: 18.4. ... ng the ELBO\n",
      "  item-119 at level 1: text: The  ... tion 18.16:\n",
      "  item-120 at level 1: formula: \n",
      "  item-121 at level 1: text: The K ... spectively:\n",
      "  item-122 at level 1: text: and i ... z 1  x ) .\n",
      "  item-123 at level 1: formula: \n",
      "  item-124 at level 1: text: The K ... onstant C :\n",
      "  item-125 at level 1: formula: \n",
      "  item-126 at level 1: picture\n",
      "    item-126 at level 2: caption: Figure 18.7 Fitted Model. a) Individual samples can be generated by sampling from the standard normal distribution Pr z ( T ) (bottom row) and then sampling z T -1 from Pr z ( T -1  z T ) = Norm z T -1 [ f T [ z T , ϕ T ] , σ 2 T I ] and so on until we reach x (ve paths shown). The estimated marginal densities (heatmap) are the aggregation of these samples and are similar to the true marginal densities (gure 18.4). b) The estimated distribution Pr z ( t -1  z t ) (brown curve) is a reasonable approximation to the true posterior of the diusion model q z ( t -1  z t ) (cyan curve) from gure 18.5. The marginal distributions Pr z ( t ) and q z ( t ) of the estimated and true models (dark blue and gray curves, respectively) are also similar.\n",
      "    item-127 at level 2: caption: Figur ... so similar.\n",
      "  item-128 at level 1: section_header: 18.4. ... ss function\n",
      "  item-129 at level 1: text: To t ... s function:\n",
      "  item-130 at level 1: section_header: reconstruction term\n",
      "  item-131 at level 1: formula: \n",
      "  item-132 at level 1: text: where ... on step t .\n",
      "  item-133 at level 1: text: Figur ... gure 18.7.\n",
      "  item-134 at level 1: picture\n",
      "  item-135 at level 1: section_header: 18.4. ... g procedure\n",
      "  item-136 at level 1: text: This  ... ed data x .\n",
      "  item-137 at level 1: text: Noteb ... usion model\n",
      "  item-138 at level 1: text: Figur ... l networks.\n",
      "  item-139 at level 1: section_header: 18.5  ... ss function\n",
      "  item-140 at level 1: text: Altho ... ion 18.29).\n",
      "  item-141 at level 1: section_header: 18.5. ... n of target\n",
      "  item-142 at level 1: text: The o ... s given by:\n",
      "  item-143 at level 1: formula: \n",
      "  item-144 at level 1: text: It fo ... dded to it:\n",
      "  item-145 at level 1: formula: \n",
      "  item-146 at level 1: text: Subst ... 8.29 gives:\n",
      "  item-147 at level 1: formula: \n",
      "  item-148 at level 1: text: where ... er, we get:\n",
      "  item-149 at level 1: formula: \n",
      "  item-150 at level 1: text: Subst ... ), we have:\n",
      "  item-151 at level 1: text: where ... e and four.\n",
      "  item-152 at level 1: formula: \n",
      "  item-153 at level 1: section_header: 18.5. ...  of network\n",
      "  item-154 at level 1: text: Now w ... reate z t :\n",
      "  item-155 at level 1: formula: \n",
      "  item-156 at level 1: text: Draft ... @gmail.com.\n",
      "  item-157 at level 1: text: Problem 18.9\n",
      "  item-158 at level 1: text: Problem 18.10\n",
      "  item-159 at level 1: section_header: Problem 18.11\n",
      "  item-160 at level 1: text: Subst ...  criterion:\n",
      "  item-161 at level 1: formula: \n",
      "  item-162 at level 1: formula: \n",
      "  item-163 at level 1: text: Subst ... mplies to:\n",
      "  item-164 at level 1: formula: \n",
      "  item-165 at level 1: formula: \n",
      "  item-166 at level 1: text: where ... tants C i .\n",
      "  item-167 at level 1: text: In pr ... ormulation:\n",
      "  item-168 at level 1: formula: \n",
      "  item-169 at level 1: section_header: 18.6 Implementation\n",
      "  item-170 at level 1: text: This  ... -consuming.\n",
      "  item-171 at level 1: section_header: Algor ... el training\n",
      "  item-172 at level 1: code: Input ... l converged\n",
      "  item-173 at level 1: section_header: Algor ... 2: Sampling\n",
      "  item-174 at level 1: code: Input ... thout noise\n",
      "  item-175 at level 1: section_header: 18.6. ... n to images\n",
      "  item-176 at level 1: text: Dius ... l position.\n",
      "  item-177 at level 1: text: A lar ... ood images.\n",
      "  item-178 at level 1: section_header: 18.6. ... ation speed\n",
      "  item-179 at level 1: text: The l ... ard process\n",
      "  item-180 at level 1: picture\n",
      "    item-180 at level 2: caption: Figure 18.9 U-Net as used in diusion models for images. The network aims to predict the noise that was added to the image. It consists of an encoder which reduces the scale and increases the number of channels and a decoder which increases the scale and reduces the number of channels. The encoder representations are concatenated to their partner in the decoder. Connections between adjacent representations consist of residual blocks, and periodic global self-attention in which every spatial position interacts with every other spatial position. A single network is used for all time steps, by passing a sinusoidal time embedding (gure 12.5) through a shallow neural network and adding the result to the channels at every spatial position at every stage of the U-Net.\n",
      "    item-181 at level 2: caption: Figur ...  the U-Net.\n",
      "  item-182 at level 1: text: with  ... ure 18.10).\n",
      "  item-183 at level 1: text: Noteb ... sion models\n",
      "  item-184 at level 1: text: Among ... ive models.\n",
      "  item-185 at level 1: section_header: 18.6. ...  generation\n",
      "  item-186 at level 1: text: If th ... ng an extra\n",
      "  item-187 at level 1: picture\n",
      "    item-187 at level 2: caption: Figure 18.10 Dierent diusion processes that are compatible with the same model. a) Five sampled trajectories of the reparameterized model superimposed on the ground truth marginal distributions. Top row represents Pr ( x ) and subsequent rows represent q ( x t ) . b) Histogram of samples generated from reparameterized model plotted alongside ground truth density curve Pr ( x ) . The same trained model is compatible with a family of diusion models (and corresponding updates in the opposite direction), including the denoising diusion implicit model (DDIM), which is deterministic and does not add noise at each step. c) Five trajectories from DDIM. d) Histogram of samples from DDIM. The same model is also compatible with accelerated diusion models that skip inference steps for increased sampling speed. e) Five trajectories from accelerated model. f) Histogram of samples from accelerated model.\n",
      "    item-188 at level 2: caption: Figur ... ated model.\n",
      "  item-189 at level 1: text: term  ... 2 to yield:\n",
      "  item-190 at level 1: formula: \n",
      "  item-191 at level 1: text: The n ... ore likely.\n",
      "  item-192 at level 1: text: Class ... information\n",
      "  item-193 at level 1: picture\n",
      "    item-193 at level 2: caption: Figure 18.11 Cascaded conditional generation based on a text prompt. a) A diusion model consisting of a series of U-Nets is used to generate a 64 × 64 image. b) This generation is conditioned on a sentence embedding computed by a language model. c) A higher resolution 256 × 256 image is generated and conditioned on the smaller image and the text encoding. d) This is repeated to create a 1024 × 1024 image. e) Final image sequence. Adapted from Saharia et al. (2022b).\n",
      "    item-194 at level 2: caption: Figur ... l. (2022b).\n",
      "  item-195 at level 1: text: Problem 18.12\n",
      "  item-196 at level 1: text: durin ... ure 15.10).\n",
      "  item-197 at level 1: section_header: 18.6. ... ion quality\n",
      "  item-198 at level 1: text: As fo ...  the widths\n",
      "  item-199 at level 1: text: of th ... ve results.\n",
      "  item-200 at level 1: text: Third ... ure 18.11).\n",
      "  item-201 at level 1: text: Combi ... me caption.\n",
      "  item-202 at level 1: section_header: 18.7 Summary\n",
      "  item-203 at level 1: text: Dius ... ormulation.\n",
      "  item-204 at level 1: text: For i ... is results.\n",
      "  item-205 at level 1: section_header: Notes\n",
      "  item-206 at level 1: text: Denoi ... At the time\n",
      "  item-207 at level 1: picture\n",
      "  item-208 at level 1: caption: Figur ... hol (2021).\n",
      "  item-209 at level 1: picture\n",
      "    item-209 at level 2: caption: transparent sculpture of a duck made out of glass\n",
      "    item-210 at level 2: caption: trans ... ut of glass\n",
      "  item-211 at level 1: picture\n",
      "  item-212 at level 1: picture\n",
      "    item-212 at level 2: caption: A brain rocketship heading towards the moon riding\n",
      "    item-213 at level 2: caption: A bra ... moon riding\n",
      "  item-214 at level 1: picture\n",
      "  item-215 at level 1: picture\n",
      "  item-216 at level 1: picture\n",
      "    item-216 at level 2: caption: An angry duck doing weightlifting at the gym heavy\n",
      "    item-217 at level 2: caption: An an ... e gym heavy\n",
      "  item-218 at level 1: picture\n",
      "  item-219 at level 1: picture\n",
      "  item-220 at level 1: caption: couple of glasses\n",
      "  item-221 at level 1: picture\n",
      "    item-221 at level 2: caption: New York skyline with Hello World\n",
      "    item-222 at level 2: caption: New Y ... Hello World\n",
      "  item-223 at level 1: caption: Figur ... l. (2022b).\n",
      "  item-224 at level 1: text: of wr ... al. (2022).\n",
      "  item-225 at level 1: text: Appli ... as a prior.\n",
      "  item-226 at level 1: text: Dier ... audio data.\n",
      "  item-227 at level 1: text: Alter ... pixelating.\n",
      "  item-228 at level 1: text: Compa ... rpretation.\n",
      "  item-229 at level 1: text: Impro ... s function.\n",
      "  item-230 at level 1: text: Kingm ...  the model.\n",
      "  item-231 at level 1: text: Ho et ... ent values.\n",
      "  item-232 at level 1: text: Impro ... r approach.\n",
      "  item-233 at level 1: text: Song  ... be applied.\n",
      "  item-234 at level 1: text: Song  ...  synthesis.\n",
      "  item-235 at level 1: text: Sampl ... usion step.\n",
      "  item-236 at level 1: text: Salim ... e sampling.\n",
      "  item-237 at level 1: text: Condi ... ic samples.\n",
      "  item-238 at level 1: text: The s ... gure 18.9).\n",
      "  item-239 at level 1: text: Exist ... ala, 2023).\n",
      "  item-240 at level 1: text: Text- ... al., 2021),\n",
      "  item-241 at level 1: text: which ... mage model.\n",
      "  item-242 at level 1: text: Conne ... approaches.\n",
      "  item-243 at level 1: section_header: Problems\n",
      "  item-244 at level 1: text: Probl ... the update:\n",
      "  item-245 at level 1: formula: \n",
      "  item-246 at level 1: text: then  ... s the same.\n",
      "  item-247 at level 1: text: Probl ... e variable:\n",
      "  item-248 at level 1: formula: \n",
      "  item-249 at level 1: text: where ...  Show that:\n",
      "  item-250 at level 1: formula: \n",
      "  item-251 at level 1: text: so we ... stribution.\n",
      "  item-252 at level 1: text: Probl ...  show that:\n",
      "  item-253 at level 1: formula: \n",
      "  item-254 at level 1: text: where ... stribution.\n",
      "  item-255 at level 1: text: Probl ... e relation:\n",
      "  item-256 at level 1: formula: \n",
      "  item-257 at level 1: text: Probl ... e relation:\n",
      "  item-258 at level 1: formula: \n",
      "  item-259 at level 1: text: Probl ... tion 18.15.\n",
      "  item-260 at level 1: text: Probl ... econd line.\n",
      "  item-261 at level 1: text: Probl ... s given by:\n",
      "  item-262 at level 1: formula: \n",
      "  item-263 at level 1: text: Subst ... tion 18.28.\n",
      "  item-264 at level 1: text: Probl ...  show that:\n",
      "  item-265 at level 1: text: Probl ...  show that:\n",
      "  item-266 at level 1: formula: \n",
      "  item-267 at level 1: formula: \n",
      "  item-268 at level 1: text: Probl ... tion 18.38.\n",
      "  item-269 at level 1: text: Probl ... n this way?\n",
      "INFO:autogen.agents.experimental.document_agent.chroma_query_engine:Loading input doc: /home/ljf1/dis/mcp_hackathon/parsed_docs/Diffusion model - Wikipedia.md\n",
      "INFO:autogen.agents.experimental.document_agent.docling_doc_ingest_agent:data_ingest_task context_variables: {'CompletedTaskCount': 1, 'DocumentsToIngest': [], 'DocumentsIngested': ['data/'], 'QueriesToRun': [Query(query_type=<QueryType.RAG_QUERY: 'RAG_QUERY'>, query='Create a list of the top 5 most important concepts and explain each in a single sentence with its importance.')], 'QueryResults': [], 'TaskInitiated': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_h3LyBK3AAz6jg7IdzYrcJINi) *****\u001b[0m\n",
      "Data Ingestion Task Completed for data/\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: TaskManagerAgent\n",
      "\u001b[0m\n",
      "\u001b[33mTaskManagerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "[Handing off to QueryAgent]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: QueryAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mQueryAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_GA7tjhAskctBj2SOQqlleNAd): execute_rag_query *****\u001b[0m\n",
      "Arguments: \n",
      "{}\n",
      "\u001b[32m**********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: _Group_Tool_Executor\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION execute_rag_query...\n",
      "Call ID: call_GA7tjhAskctBj2SOQqlleNAd\n",
      "Input arguments: {}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_GA7tjhAskctBj2SOQqlleNAd) *****\u001b[0m\n",
      "1. **Diffusion Models**: These models transform data through a series of latent variables by blending with random noise, which is crucial for generating high-quality images through a reverse denoising process.\n",
      "\n",
      "2. **Classifier-Free Guidance**: This technique incorporates class information directly into the main model, allowing for both conditional and unconditional data generation, enhancing the model's flexibility and output quality.\n",
      "\n",
      "3. **Cascaded Conditional Generation**: By using a series of U-Nets to progressively generate higher-resolution images conditioned on text or smaller images, this approach significantly improves the resolution and quality of generated images.\n",
      "\n",
      "4. **Noise Schedule Modification**: Adjusting the noise schedule in the forward process can enhance the quality of generated images, particularly when sampling with fewer steps, making the process more efficient.\n",
      "\n",
      "5. **Stochastic Nature of Diffusion Models**: The inherent randomness allows for the generation of diverse outputs from the same input conditions, which is essential for creating varied and realistic images.\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: QueryAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mQueryAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Here are the top 5 most important concepts:\n",
      "\n",
      "1. **Diffusion Models**: These models transform data through a series of latent variables by blending with random noise, which is crucial for generating high-quality images through a reverse denoising process. This is important because it allows for the creation of realistic images from abstract data.\n",
      "\n",
      "2. **Classifier-Free Guidance**: This technique incorporates class information directly into the main model, allowing for both conditional and unconditional data generation, enhancing the model's flexibility and output quality. Its significance lies in improving the versatility of generative models.\n",
      "\n",
      "3. **Cascaded Conditional Generation**: By using a series of U-Nets to progressively generate higher-resolution images conditioned on text or smaller images, this approach significantly improves the resolution and quality of generated images. This is vital for applications requiring high fidelity in visual outputs.\n",
      "\n",
      "4. **Noise Schedule Modification**: Adjusting the noise schedule in the forward process can enhance the quality of generated images, particularly when sampling with fewer steps, making the process more efficient. This is important for optimizing computational resources while maintaining output quality.\n",
      "\n",
      "5. **Stochastic Nature of Diffusion Models**: The inherent randomness allows for the generation of diverse outputs from the same input conditions, which is essential for creating varied and realistic images. This diversity is crucial for applications in creative fields where uniqueness is valued.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: TaskManagerAgent\n",
      "\u001b[0m\n",
      "\u001b[33mTaskManagerAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "[Handing off to SummaryAgent]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SummaryAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSummaryAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Ingestions:\n",
      "1. Ingested documents from data/.\n",
      "\n",
      "Queries:\n",
      "1. Query: Create a list of the top 5 most important concepts and explain each in a single sentence with its importance.\n",
      "   Answer: \n",
      "   1. **Diffusion Models**: These models transform data through a series of latent variables by blending with random noise, which is crucial for generating high-quality images through a reverse denoising process. This is important because it allows for the creation of realistic images from abstract data.\n",
      "   2. **Classifier-Free Guidance**: This technique incorporates class information directly into the main model, allowing for both conditional and unconditional data generation, enhancing the model's flexibility and output quality. Its significance lies in improving the versatility of generative models.\n",
      "   3. **Cascaded Conditional Generation**: By using a series of U-Nets to progressively generate higher-resolution images conditioned on text or smaller images, this approach significantly improves the resolution and quality of generated images. This is vital for applications requiring high fidelity in visual outputs.\n",
      "   4. **Noise Schedule Modification**: Adjusting the noise schedule in the forward process can enhance the quality of generated images, particularly when sampling with fewer steps, making the process more efficient. This is important for optimizing computational resources while maintaining output quality.\n",
      "   5. **Stochastic Nature of Diffusion Models**: The inherent randomness allows for the generation of diverse outputs from the same input conditions, which is essential for creating varied and realistic images. This diversity is crucial for applications in creative fields where uniqueness is valued.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (efbd9afa-4653-4700-b95c-1109114b6e13): No next speaker selected\u001b[0m\n",
      "\u001b[33mDocAgent\u001b[0m (to user):\n",
      "\n",
      "Ingestions:\n",
      "1. Ingested documents from data/.\n",
      "\n",
      "Queries:\n",
      "1. Query: Create a list of the top 5 most important concepts and explain each in a single sentence with its importance.\n",
      "   Answer: \n",
      "   1. **Diffusion Models**: These models transform data through a series of latent variables by blending with random noise, which is crucial for generating high-quality images through a reverse denoising process. This is important because it allows for the creation of realistic images from abstract data.\n",
      "   2. **Classifier-Free Guidance**: This technique incorporates class information directly into the main model, allowing for both conditional and unconditional data generation, enhancing the model's flexibility and output quality. Its significance lies in improving the versatility of generative models.\n",
      "   3. **Cascaded Conditional Generation**: By using a series of U-Nets to progressively generate higher-resolution images conditioned on text or smaller images, this approach significantly improves the resolution and quality of generated images. This is vital for applications requiring high fidelity in visual outputs.\n",
      "   4. **Noise Schedule Modification**: Adjusting the noise schedule in the forward process can enhance the quality of generated images, particularly when sampling with fewer steps, making the process more efficient. This is important for optimizing computational resources while maintaining output quality.\n",
      "   5. **Stochastic Nature of Diffusion Models**: The inherent randomness allows for the generation of diverse outputs from the same input conditions, which is essential for creating varied and realistic images. This diversity is crucial for applications in creative fields where uniqueness is valued.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (4cfa6ad0-8cbd-4c1a-908d-d3958735d307): Maximum turns (1) reached\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "doc_agent = DocAgent(llm_config=llm_config, collection_name='summarise')\n",
    "run_response = doc_agent.run(\n",
    "    message = f\"ingest all documents in {DOC_PATH} and do {QUERY}.\",\n",
    "    max_turns=1,\n",
    "    \n",
    ")\n",
    "run_response.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_response.messages[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestions:\n",
      "1. Ingested documents from data/.\n",
      "\n",
      "Queries:\n",
      "1. Query: Create a list of the top 5 most important concepts and explain each in a single sentence with its importance.\n",
      "   Answer: \n",
      "   1. **Diffusion Models**: These models transform data through a series of latent variables by blending with random noise, which is crucial for generating high-quality images through a reverse denoising process. This is important because it allows for the creation of realistic images from abstract data.\n",
      "   2. **Classifier-Free Guidance**: This technique incorporates class information directly into the main model, allowing for both conditional and unconditional data generation, enhancing the model's flexibility and output quality. Its significance lies in improving the versatility of generative models.\n",
      "   3. **Cascaded Conditional Generation**: By using a series of U-Nets to progressively generate higher-resolution images conditioned on text or smaller images, this approach significantly improves the resolution and quality of generated images. This is vital for applications requiring high fidelity in visual outputs.\n",
      "   4. **Noise Schedule Modification**: Adjusting the noise schedule in the forward process can enhance the quality of generated images, particularly when sampling with fewer steps, making the process more efficient. This is important for optimizing computational resources while maintaining output quality.\n",
      "   5. **Stochastic Nature of Diffusion Models**: The inherent randomness allows for the generation of diverse outputs from the same input conditions, which is essential for creating varied and realistic images. This diversity is crucial for applications in creative fields where uniqueness is valued.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalizer = ConversableAgent(\n",
    "    name=\"Finalizer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your job is to clean up the results from the DocAgent. Return the results in a list format, including as much detail as possible. Return ONLY the list with no other additional text.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to Finalizer):\n",
      "\n",
      "Ingestions:\n",
      "1. Ingested documents from data/.\n",
      "\n",
      "Queries:\n",
      "1. Query: Create a list of the top 5 most important concepts and explain each in a single sentence with its importance.\n",
      "   Answer: \n",
      "   1. **Diffusion Models**: These models transform data through a series of latent variables by blending with random noise, which is crucial for generating high-quality images through a reverse denoising process. This is important because it allows for the creation of realistic images from abstract data.\n",
      "   2. **Classifier-Free Guidance**: This technique incorporates class information directly into the main model, allowing for both conditional and unconditional data generation, enhancing the model's flexibility and output quality. Its significance lies in improving the versatility of generative models.\n",
      "   3. **Cascaded Conditional Generation**: By using a series of U-Nets to progressively generate higher-resolution images conditioned on text or smaller images, this approach significantly improves the resolution and quality of generated images. This is vital for applications requiring high fidelity in visual outputs.\n",
      "   4. **Noise Schedule Modification**: Adjusting the noise schedule in the forward process can enhance the quality of generated images, particularly when sampling with fewer steps, making the process more efficient. This is important for optimizing computational resources while maintaining output quality.\n",
      "   5. **Stochastic Nature of Diffusion Models**: The inherent randomness allows for the generation of diverse outputs from the same input conditions, which is essential for creating varied and realistic images. This diversity is crucial for applications in creative fields where uniqueness is valued.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mFinalizer\u001b[0m (to user):\n",
      "\n",
      "1. **Diffusion Models**: These models transform data through a series of latent variables by blending with random noise, which is crucial for generating high-quality images through a reverse denoising process. This is important because it allows for the creation of realistic images from abstract data.\n",
      "2. **Classifier-Free Guidance**: This technique incorporates class information directly into the main model, allowing for both conditional and unconditional data generation, enhancing the model's flexibility and output quality. Its significance lies in improving the versatility of generative models.\n",
      "3. **Cascaded Conditional Generation**: By using a series of U-Nets to progressively generate higher-resolution images conditioned on text or smaller images, this approach significantly improves the resolution and quality of generated images. This is vital for applications requiring high fidelity in visual outputs.\n",
      "4. **Noise Schedule Modification**: Adjusting the noise schedule in the forward process can enhance the quality of generated images, particularly when sampling with fewer steps, making the process more efficient. This is important for optimizing computational resources while maintaining output quality.\n",
      "5. **Stochastic Nature of Diffusion Models**: The inherent randomness allows for the generation of diverse outputs from the same input conditions, which is essential for creating varied and realistic images. This diversity is crucial for applications in creative fields where uniqueness is valued.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (aed4e749-8890-4542-9856-fef14b88f42a): Maximum turns (1) reached\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "finalizer_response = finalizer.run(\n",
    "    message=result,\n",
    "    max_turns=1,\n",
    ")\n",
    "finalizer_response.process()\n",
    "final_result = finalizer_response.messages[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Diffusion Models**: These models transform data through a series of latent variables by blending with random noise, which is crucial for generating high-quality images through a reverse denoising process. This is important because it allows for the creation of realistic images from abstract data.\n",
      "2. **Classifier-Free Guidance**: This technique incorporates class information directly into the main model, allowing for both conditional and unconditional data generation, enhancing the model's flexibility and output quality. Its significance lies in improving the versatility of generative models.\n",
      "3. **Cascaded Conditional Generation**: By using a series of U-Nets to progressively generate higher-resolution images conditioned on text or smaller images, this approach significantly improves the resolution and quality of generated images. This is vital for applications requiring high fidelity in visual outputs.\n",
      "4. **Noise Schedule Modification**: Adjusting the noise schedule in the forward process can enhance the quality of generated images, particularly when sampling with fewer steps, making the process more efficient. This is important for optimizing computational resources while maintaining output quality.\n",
      "5. **Stochastic Nature of Diffusion Models**: The inherent randomness allows for the generation of diverse outputs from the same input conditions, which is essential for creating varied and realistic images. This diversity is crucial for applications in creative fields where uniqueness is valued.\n"
     ]
    }
   ],
   "source": [
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
